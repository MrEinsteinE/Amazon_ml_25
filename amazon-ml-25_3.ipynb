{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: torch in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: optuna in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from optuna) (1.17.0)\n",
      "Requirement already satisfied: colorlog in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: tomli in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from alembic>=1.5.0->optuna) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scikit-learn torch torchvision transformers pillow requests tqdm xgboost lightgbm optuna matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "NumPy version: 2.0.1\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 1: ENVIRONMENT SETUP & DATA LOADING\n",
    "# Amazon ML Challenge 2025 - Product Price Prediction\n",
    "# ============================================================\n",
    "\n",
    "# Import core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For image processing\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Machine learning libraries (we'll use more later)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/dataset/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# LOAD DATASETS\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load training data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/dataset/train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/dataset/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m sample_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/dataset/sample_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\91850\\anaconda3\\envs\\amazon_ml\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/dataset/train.csv'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD DATASETS\n",
    "# ============================================================\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv('/kaggle/input/dataset/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/dataset/test.csv')\n",
    "sample_test = pd.read_csv('/kaggle/input/dataset/sample_test.csv')\n",
    "sample_output = pd.read_csv('/kaggle/input/dataset/sample_test_out.csv')\n",
    "\n",
    "print(\"âœ… Data loaded successfully!\")\n",
    "print(f\"\\nTraining set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Sample test shape: {sample_test.shape}\")\n",
    "print(f\"Sample output shape: {sample_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BASIC DATA EXPLORATION\n",
    "# ============================================================\n",
    "\n",
    "# Display first few rows\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING DATA - FIRST 5 ROWS\")\n",
    "print(\"=\" * 80)\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA TYPES AND MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "print(train_df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "missing_train = train_df.isnull().sum()\n",
    "missing_pct = (missing_train / len(train_df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_train.index,\n",
    "    'Missing_Count': missing_train.values,\n",
    "    'Missing_Percentage': missing_pct.values\n",
    "})\n",
    "display(missing_df)\n",
    "\n",
    "# Check test data missing values\n",
    "print(\"\\nTest Data Missing Values:\")\n",
    "print(test_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TARGET VARIABLE ANALYSIS (PRICE)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PRICE DISTRIBUTION STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(train_df['price'].describe())\n",
    "\n",
    "# Additional statistics\n",
    "print(f\"\\nMin Price: ${train_df['price'].min():.2f}\")\n",
    "print(f\"Max Price: ${train_df['price'].max():.2f}\")\n",
    "print(f\"Mean Price: ${train_df['price'].mean():.2f}\")\n",
    "print(f\"Median Price: ${train_df['price'].median():.2f}\")\n",
    "print(f\"Std Dev: ${train_df['price'].std():.2f}\")\n",
    "print(f\"Skewness: {train_df['price'].skew():.2f}\")\n",
    "print(f\"Kurtosis: {train_df['price'].kurtosis():.2f}\")\n",
    "\n",
    "# Check for negative prices (data quality check)\n",
    "negative_prices = train_df[train_df['price'] <= 0]\n",
    "print(f\"\\nâš ï¸ Rows with negative/zero prices: {len(negative_prices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRICE DISTRIBUTION VISUALIZATIONS\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Original price distribution\n",
    "axes[0, 0].hist(train_df['price'], bins=100, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Price Distribution (Original)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Price ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-transformed price (for better visualization)\n",
    "axes[0, 1].hist(np.log1p(train_df['price']), bins=100, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Price Distribution (Log-Transformed)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Log(Price + 1)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot for outlier detection\n",
    "axes[1, 0].boxplot(train_df['price'], vert=True, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightgreen', alpha=0.7))\n",
    "axes[1, 0].set_title('Price Box Plot (Outlier Detection)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Price ($)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot for normality check\n",
    "from scipy import stats\n",
    "stats.probplot(train_df['price'], dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Normality Check)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Price distribution visualizations created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CATALOG CONTENT ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CATALOG CONTENT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Text length analysis\n",
    "train_df['catalog_length'] = train_df['catalog_content'].str.len()\n",
    "\n",
    "print(f\"Average catalog content length: {train_df['catalog_length'].mean():.0f} characters\")\n",
    "print(f\"Min length: {train_df['catalog_length'].min()}\")\n",
    "print(f\"Max length: {train_df['catalog_length'].max()}\")\n",
    "print(f\"Median length: {train_df['catalog_length'].median():.0f}\")\n",
    "\n",
    "# Sample catalog content\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE CATALOG CONTENT (First 3 products)\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in train_df.head(3).iterrows():\n",
    "    print(f\"\\nProduct {idx + 1} (Price: ${row['price']:.2f}):\")\n",
    "    print(f\"Catalog: {row['catalog_content'][:300]}...\")\n",
    "    print(f\"Image URL: {row['image_link']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMAGE LINK ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"IMAGE LINK ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for missing image links\n",
    "missing_images_train = train_df['image_link'].isnull().sum()\n",
    "missing_images_test = test_df['image_link'].isnull().sum()\n",
    "\n",
    "print(f\"Missing image links in training: {missing_images_train} ({missing_images_train/len(train_df)*100:.2f}%)\")\n",
    "print(f\"Missing image links in test: {missing_images_test} ({missing_images_test/len(test_df)*100:.2f}%)\")\n",
    "\n",
    "# Check image URL patterns\n",
    "print(\"\\nSample image URLs:\")\n",
    "display(train_df['image_link'].head(10))\n",
    "\n",
    "# Check unique domains\n",
    "train_df['image_domain'] = train_df['image_link'].str.extract(r'https?://([^/]+)/')\n",
    "print(f\"\\nUnique image domains: {train_df['image_domain'].nunique()}\")\n",
    "print(\"\\nTop image domains:\")\n",
    "print(train_df['image_domain'].value_counts().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORRELATION ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create correlation between text length and price\n",
    "correlation_data = pd.DataFrame({\n",
    "    'catalog_length': train_df['catalog_length'],\n",
    "    'price': train_df['price'],\n",
    "    'log_price': np.log1p(train_df['price'])\n",
    "})\n",
    "\n",
    "# Calculate correlations\n",
    "corr_matrix = correlation_data.corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "display(corr_matrix)\n",
    "\n",
    "# Visualize correlation\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, fmt='.3f')\n",
    "plt.title('Correlation: Catalog Length vs Price', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(train_df['catalog_length'], train_df['price'], alpha=0.3, s=10)\n",
    "plt.xlabel('Catalog Content Length')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Catalog Length vs Price')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(train_df['catalog_length'], np.log1p(train_df['price']), alpha=0.3, s=10)\n",
    "plt.xlabel('Catalog Content Length')\n",
    "plt.ylabel('Log(Price + 1)')\n",
    "plt.title('Catalog Length vs Log(Price)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRICE RANGE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "# Create price bins for analysis\n",
    "train_df['price_range'] = pd.cut(train_df['price'], \n",
    "                                   bins=[0, 10, 25, 50, 100, 500, 1000, float('inf')],\n",
    "                                   labels=['$0-10', '$10-25', '$25-50', '$50-100', \n",
    "                                          '$100-500', '$500-1000', '$1000+'])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PRICE RANGE DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "price_range_dist = train_df['price_range'].value_counts().sort_index()\n",
    "print(price_range_dist)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "price_range_dist.plot(kind='bar', color='teal', alpha=0.7, edgecolor='black')\n",
    "plt.title('Product Count by Price Range', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Price Range')\n",
    "plt.ylabel('Number of Products')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA QUALITY CHECKS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = train_df.duplicated().sum()\n",
    "print(f\"Duplicate rows in training: {duplicates}\")\n",
    "\n",
    "# Check sample_id uniqueness\n",
    "print(f\"Unique sample_ids in train: {train_df['sample_id'].nunique()} / {len(train_df)}\")\n",
    "print(f\"Unique sample_ids in test: {test_df['sample_id'].nunique()} / {len(test_df)}\")\n",
    "\n",
    "# Check if test sample_ids match expected output\n",
    "test_ids = set(test_df['sample_id'].values)\n",
    "sample_ids = set(sample_test['sample_id'].values)\n",
    "output_ids = set(sample_output['sample_id'].values)\n",
    "\n",
    "print(f\"\\nSample test IDs match output: {sample_ids == output_ids}\")\n",
    "print(f\"Sample test size: {len(sample_test)}\")\n",
    "print(f\"Sample output size: {len(sample_output)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET SUMMARY FOR AMAZON ML CHALLENGE 2025\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = {\n",
    "    'Metric': [\n",
    "        'Training Samples',\n",
    "        'Test Samples',\n",
    "        'Total Samples',\n",
    "        'Features',\n",
    "        'Target Variable',\n",
    "        'Min Price',\n",
    "        'Max Price',\n",
    "        'Mean Price',\n",
    "        'Median Price',\n",
    "        'Price Std Dev',\n",
    "        'Missing Images (Train)',\n",
    "        'Missing Images (Test)',\n",
    "        'Avg Catalog Length'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{len(train_df):,}\",\n",
    "        f\"{len(test_df):,}\",\n",
    "        f\"{len(train_df) + len(test_df):,}\",\n",
    "        f\"{len(train_df.columns) - 1}\",  # Excluding target\n",
    "        \"price (continuous)\",\n",
    "        f\"${train_df['price'].min():.2f}\",\n",
    "        f\"${train_df['price'].max():.2f}\",\n",
    "        f\"${train_df['price'].mean():.2f}\",\n",
    "        f\"${train_df['price'].median():.2f}\",\n",
    "        f\"${train_df['price'].std():.2f}\",\n",
    "        f\"{missing_images_train} ({missing_images_train/len(train_df)*100:.2f}%)\",\n",
    "        f\"{missing_images_test} ({missing_images_test/len(test_df)*100:.2f}%)\",\n",
    "        f\"{train_df['catalog_length'].mean():.0f} chars\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\nâœ… STEP 1 COMPLETED: Data loaded and explored!\")\n",
    "print(\"ðŸ“Š Next steps: Feature extraction, text processing, and image handling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: FEATURE ENGINEERING & TEXT PROCESSING\n",
    "# Amazon ML Challenge 2025 - Extract Rich Features\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: FEATURE ENGINEERING FROM CATALOG CONTENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS FOR TEXT EXTRACTION\n",
    "# ============================================================\n",
    "\n",
    "def extract_item_name(text):\n",
    "    \"\"\"Extract item name from catalog content\"\"\"\n",
    "    match = re.search(r'Item Name:\\s*(.+?)(?:\\n|$)', text)\n",
    "    return match.group(1).strip() if match else ''\n",
    "\n",
    "def extract_value_unit(text):\n",
    "    \"\"\"Extract numerical value and unit (e.g., 12.0 Fl Oz)\"\"\"\n",
    "    value_match = re.search(r'Value:\\s*(\\d+\\.?\\d*)', text)\n",
    "    unit_match = re.search(r'Unit:\\s*(.+?)(?:\\n|$)', text)\n",
    "    \n",
    "    value = float(value_match.group(1)) if value_match else 0.0\n",
    "    unit = unit_match.group(1).strip() if unit_match else ''\n",
    "    \n",
    "    return value, unit\n",
    "\n",
    "def extract_pack_count(text):\n",
    "    \"\"\"Extract pack count from text (e.g., Pack of 6, 4-Pack)\"\"\"\n",
    "    patterns = [\n",
    "        r'[Pp]ack\\s+of\\s+(\\d+)',\n",
    "        r'\\(Pack\\s+of\\s+(\\d+)\\)',\n",
    "        r'(\\d+)[- ][Pp]ack',\n",
    "        r'(\\d+)\\s*[Cc]ount'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    return 1  # Default to 1 if no pack info found\n",
    "\n",
    "def extract_all_numbers(text):\n",
    "    \"\"\"Extract all numeric values from text\"\"\"\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', text)\n",
    "    return [float(n) for n in numbers] if numbers else []\n",
    "\n",
    "def extract_weight_volume(text):\n",
    "    \"\"\"Extract weight or volume with units\"\"\"\n",
    "    # Common patterns for weight/volume\n",
    "    patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*(oz|ounce|lb|pound|kg|kilogram|g|gram)',\n",
    "        r'(\\d+\\.?\\d*)\\s*(ml|milliliter|l|liter|fl\\s*oz|gallon)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return float(match.group(1)), match.group(2).lower()\n",
    "    return 0.0, ''\n",
    "\n",
    "def extract_brand(text):\n",
    "    \"\"\"Extract potential brand name (first few words or capitalized words)\"\"\"\n",
    "    item_name = extract_item_name(text)\n",
    "    if item_name:\n",
    "        # Get first 2-3 words as potential brand\n",
    "        words = item_name.split()[:3]\n",
    "        return ' '.join(words)\n",
    "    return ''\n",
    "\n",
    "def count_bullet_points(text):\n",
    "    \"\"\"Count number of bullet points in description\"\"\"\n",
    "    return len(re.findall(r'Bullet Point \\d+:', text))\n",
    "\n",
    "def calculate_total_quantity(text):\n",
    "    \"\"\"Calculate total quantity (pack_count * unit_value)\"\"\"\n",
    "    pack_count = extract_pack_count(text)\n",
    "    value, unit = extract_value_unit(text)\n",
    "    return pack_count * value if value > 0 else pack_count\n",
    "\n",
    "print(\"âœ… Helper functions defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# APPLY FEATURE EXTRACTION TO TRAINING DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXTRACTING FEATURES FROM TRAINING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create feature columns\n",
    "train_df['item_name'] = train_df['catalog_content'].apply(extract_item_name)\n",
    "train_df['brand'] = train_df['catalog_content'].apply(extract_brand)\n",
    "train_df['unit_value'], train_df['unit_type'] = zip(*train_df['catalog_content'].apply(extract_value_unit))\n",
    "train_df['pack_count'] = train_df['catalog_content'].apply(extract_pack_count)\n",
    "train_df['total_quantity'] = train_df['catalog_content'].apply(calculate_total_quantity)\n",
    "train_df['weight_value'], train_df['weight_unit'] = zip(*train_df['catalog_content'].apply(extract_weight_volume))\n",
    "train_df['num_bullet_points'] = train_df['catalog_content'].apply(count_bullet_points)\n",
    "train_df['word_count'] = train_df['catalog_content'].apply(lambda x: len(x.split()))\n",
    "train_df['char_count'] = train_df['catalog_content'].apply(len)\n",
    "train_df['num_numbers'] = train_df['catalog_content'].apply(lambda x: len(extract_all_numbers(x)))\n",
    "\n",
    "# Text complexity features\n",
    "train_df['avg_word_length'] = train_df['catalog_content'].apply(\n",
    "    lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0\n",
    ")\n",
    "\n",
    "print(\"âœ… Features extracted successfully!\")\n",
    "print(f\"\\nNew feature columns added: {len(train_df.columns) - 4}\")\n",
    "print(\"\\nFeature columns:\")\n",
    "for col in train_df.columns[4:]:  # Skip original columns\n",
    "    print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DISPLAY EXTRACTED FEATURES (SAMPLE)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE EXTRACTED FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select important columns to display\n",
    "feature_cols = ['sample_id', 'item_name', 'brand', 'unit_value', 'unit_type', \n",
    "                'pack_count', 'total_quantity', 'num_bullet_points', 'price']\n",
    "\n",
    "print(\"\\nFirst 10 products with extracted features:\")\n",
    "display(train_df[feature_cols].head(10))\n",
    "\n",
    "# Show statistics for numerical features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NUMERICAL FEATURE STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "numerical_features = ['unit_value', 'pack_count', 'total_quantity', 'weight_value',\n",
    "                      'num_bullet_points', 'word_count', 'char_count', 'num_numbers', \n",
    "                      'avg_word_length']\n",
    "\n",
    "display(train_df[numerical_features].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UNIT TYPE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UNIT TYPE DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze unit types\n",
    "unit_counts = train_df['unit_type'].value_counts()\n",
    "print(f\"\\nUnique unit types: {len(unit_counts)}\")\n",
    "print(\"\\nTop 20 unit types:\")\n",
    "print(unit_counts.head(20))\n",
    "\n",
    "# Standardize common units\n",
    "def standardize_unit(unit):\n",
    "    \"\"\"Standardize unit types to common categories\"\"\"\n",
    "    unit = unit.lower().strip()\n",
    "    \n",
    "    # Weight units\n",
    "    if unit in ['oz', 'ounce', 'ounces']:\n",
    "        return 'oz'\n",
    "    elif unit in ['lb', 'pound', 'pounds']:\n",
    "        return 'lb'\n",
    "    elif unit in ['kg', 'kilogram', 'kilograms']:\n",
    "        return 'kg'\n",
    "    elif unit in ['g', 'gram', 'grams']:\n",
    "        return 'g'\n",
    "    \n",
    "    # Volume units\n",
    "    elif unit in ['ml', 'milliliter', 'milliliters']:\n",
    "        return 'ml'\n",
    "    elif unit in ['l', 'liter', 'liters']:\n",
    "        return 'l'\n",
    "    elif 'fl oz' in unit or 'fluid ounce' in unit:\n",
    "        return 'fl_oz'\n",
    "    elif unit in ['gallon', 'gallons', 'gal']:\n",
    "        return 'gallon'\n",
    "    \n",
    "    # Count units\n",
    "    elif unit in ['count', 'piece', 'pieces', 'ct']:\n",
    "        return 'count'\n",
    "    \n",
    "    return unit\n",
    "\n",
    "train_df['unit_type_std'] = train_df['unit_type'].apply(standardize_unit)\n",
    "\n",
    "print(\"\\nStandardized unit types:\")\n",
    "print(train_df['unit_type_std'].value_counts().head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CATEGORY/PRODUCT TYPE EXTRACTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRODUCT CATEGORY IDENTIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define common food/product categories\n",
    "def identify_category(text):\n",
    "    \"\"\"Identify product category from text\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    categories = {\n",
    "        'sauce': ['sauce', 'salsa', 'dressing', 'condiment'],\n",
    "        'cookie': ['cookie', 'biscuit', 'wafer'],\n",
    "        'soup': ['soup', 'broth', 'stew'],\n",
    "        'cheese': ['cheese', 'cheddar', 'parmesan'],\n",
    "        'wine': ['wine', 'cooking wine'],\n",
    "        'snack': ['chip', 'snack', 'cracker', 'popcorn'],\n",
    "        'beverage': ['coffee', 'tea', 'juice', 'drink', 'soda'],\n",
    "        'pasta': ['pasta', 'noodle', 'spaghetti', 'macaroni'],\n",
    "        'oil': ['oil', 'olive oil', 'vegetable oil'],\n",
    "        'spice': ['spice', 'seasoning', 'powder', 'herb'],\n",
    "        'cereal': ['cereal', 'granola', 'oatmeal'],\n",
    "        'candy': ['candy', 'chocolate', 'sweet', 'gummy'],\n",
    "        'baking': ['flour', 'sugar', 'baking', 'mix'],\n",
    "        'canned': ['canned', 'can of'],\n",
    "        'dried': ['dried', 'dehydrated']\n",
    "    }\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in text_lower:\n",
    "                return category\n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "train_df['category'] = train_df['catalog_content'].apply(identify_category)\n",
    "\n",
    "print(\"Category distribution:\")\n",
    "category_dist = train_df['category'].value_counts()\n",
    "print(category_dist)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "category_dist.head(15).plot(kind='barh', color='teal', edgecolor='black', alpha=0.7)\n",
    "plt.title('Top 15 Product Categories', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Category')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRICE ANALYSIS BY FEATURES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRICE ANALYSIS BY KEY FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Price by pack count\n",
    "print(\"\\n1. Average Price by Pack Count:\")\n",
    "price_by_pack = train_df.groupby('pack_count')['price'].agg(['mean', 'median', 'count']).round(2)\n",
    "display(price_by_pack.head(15))\n",
    "\n",
    "# Price by category\n",
    "print(\"\\n2. Average Price by Category:\")\n",
    "price_by_category = train_df.groupby('category')['price'].agg(['mean', 'median', 'count']).round(2)\n",
    "price_by_category = price_by_category.sort_values('mean', ascending=False)\n",
    "display(price_by_category)\n",
    "\n",
    "# Price by unit type\n",
    "print(\"\\n3. Average Price by Unit Type (Top 10):\")\n",
    "price_by_unit = train_df[train_df['unit_type_std'] != ''].groupby('unit_type_std')['price'].agg(['mean', 'median', 'count']).round(2)\n",
    "price_by_unit = price_by_unit.sort_values('mean', ascending=False)\n",
    "display(price_by_unit.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORRELATION ANALYSIS WITH NEW FEATURES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS: NEW FEATURES VS PRICE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select numerical features for correlation\n",
    "correlation_features = ['unit_value', 'pack_count', 'total_quantity', 'weight_value',\n",
    "                        'num_bullet_points', 'word_count', 'char_count', \n",
    "                        'num_numbers', 'avg_word_length', 'price']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = train_df[correlation_features].corr()\n",
    "\n",
    "# Display correlations with price\n",
    "price_corr = corr_matrix['price'].sort_values(ascending=False)\n",
    "print(\"\\nFeature Correlations with Price:\")\n",
    "print(price_corr)\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdYlGn', center=0, \n",
    "            square=True, linewidths=1, fmt='.3f', cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZE KEY RELATIONSHIPS\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Pack Count vs Price\n",
    "axes[0, 0].scatter(train_df['pack_count'], train_df['price'], alpha=0.3, s=10)\n",
    "axes[0, 0].set_xlabel('Pack Count')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "axes[0, 0].set_title('Pack Count vs Price')\n",
    "axes[0, 0].set_xlim(0, 50)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Total Quantity vs Log Price\n",
    "axes[0, 1].scatter(train_df['total_quantity'], np.log1p(train_df['price']), alpha=0.3, s=10, color='coral')\n",
    "axes[0, 1].set_xlabel('Total Quantity')\n",
    "axes[0, 1].set_ylabel('Log(Price + 1)')\n",
    "axes[0, 1].set_title('Total Quantity vs Log(Price)')\n",
    "axes[0, 1].set_xlim(0, 500)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Word Count vs Price\n",
    "axes[0, 2].scatter(train_df['word_count'], train_df['price'], alpha=0.3, s=10, color='green')\n",
    "axes[0, 2].set_xlabel('Word Count')\n",
    "axes[0, 2].set_ylabel('Price ($)')\n",
    "axes[0, 2].set_title('Word Count vs Price')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Number of Bullet Points vs Price\n",
    "bp_price = train_df.groupby('num_bullet_points')['price'].mean()\n",
    "axes[1, 0].bar(bp_price.index, bp_price.values, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Number of Bullet Points')\n",
    "axes[1, 0].set_ylabel('Average Price ($)')\n",
    "axes[1, 0].set_title('Bullet Points vs Average Price')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Category vs Average Price (Top 10)\n",
    "category_price = train_df.groupby('category')['price'].mean().sort_values(ascending=False).head(10)\n",
    "axes[1, 1].barh(category_price.index, category_price.values, color='teal', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Average Price ($)')\n",
    "axes[1, 1].set_ylabel('Category')\n",
    "axes[1, 1].set_title('Top 10 Categories by Price')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 6. Unit Value Distribution\n",
    "axes[1, 2].hist(train_df[train_df['unit_value'] > 0]['unit_value'], bins=50, \n",
    "                color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1, 2].set_xlabel('Unit Value')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].set_title('Unit Value Distribution')\n",
    "axes[1, 2].set_xlim(0, 100)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# APPLY SAME FEATURE EXTRACTION TO TEST DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXTRACTING FEATURES FROM TEST DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Apply same transformations to test data\n",
    "test_df['item_name'] = test_df['catalog_content'].apply(extract_item_name)\n",
    "test_df['brand'] = test_df['catalog_content'].apply(extract_brand)\n",
    "test_df['unit_value'], test_df['unit_type'] = zip(*test_df['catalog_content'].apply(extract_value_unit))\n",
    "test_df['pack_count'] = test_df['catalog_content'].apply(extract_pack_count)\n",
    "test_df['total_quantity'] = test_df['catalog_content'].apply(calculate_total_quantity)\n",
    "test_df['weight_value'], test_df['weight_unit'] = zip(*test_df['catalog_content'].apply(extract_weight_volume))\n",
    "test_df['num_bullet_points'] = test_df['catalog_content'].apply(count_bullet_points)\n",
    "test_df['word_count'] = test_df['catalog_content'].apply(lambda x: len(x.split()))\n",
    "test_df['char_count'] = test_df['catalog_content'].apply(len)\n",
    "test_df['num_numbers'] = test_df['catalog_content'].apply(lambda x: len(extract_all_numbers(x)))\n",
    "test_df['avg_word_length'] = test_df['catalog_content'].apply(\n",
    "    lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0\n",
    ")\n",
    "test_df['unit_type_std'] = test_df['unit_type'].apply(standardize_unit)\n",
    "test_df['category'] = test_df['catalog_content'].apply(identify_category)\n",
    "\n",
    "print(\"âœ… Features extracted from test data!\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nSample test features:\")\n",
    "display(test_df[['sample_id', 'brand', 'pack_count', 'total_quantity', 'category']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENCODING CATEGORICAL FEATURES - HANDLE UNSEEN VALUES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENCODING CATEGORICAL FEATURES (FIXED VERSION)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def safe_label_encode_v2(train_series, test_series, column_name):\n",
    "    \"\"\"\n",
    "    Safely encode labels by fitting on combined unique values\n",
    "    \"\"\"\n",
    "    # Fill NaN with 'unknown'\n",
    "    train_clean = train_series.fillna('unknown').astype(str)\n",
    "    test_clean = test_series.fillna('unknown').astype(str)\n",
    "    \n",
    "    # Get all unique values from both train and test\n",
    "    all_unique_values = list(set(train_clean.unique()) | set(test_clean.unique()))\n",
    "    \n",
    "    # Add 'unknown' if not present\n",
    "    if 'unknown' not in all_unique_values:\n",
    "        all_unique_values.append('unknown')\n",
    "    \n",
    "    # Create and fit encoder on all possible values\n",
    "    le = LabelEncoder()\n",
    "    le.fit(all_unique_values)\n",
    "    \n",
    "    # Transform both datasets\n",
    "    train_encoded = le.transform(train_clean)\n",
    "    test_encoded = le.transform(test_clean)\n",
    "    \n",
    "    # Count how many test values were not in training\n",
    "    train_set = set(train_clean.unique())\n",
    "    test_set = set(test_clean.unique())\n",
    "    unseen = test_set - train_set\n",
    "    \n",
    "    if unseen:\n",
    "        num_unseen = sum(test_clean.isin(unseen))\n",
    "        print(f\"  âš ï¸  {column_name}: {len(unseen)} unique unseen categories ({num_unseen} total occurrences)\")\n",
    "        print(f\"      Examples: {list(unseen)[:3]}\")\n",
    "    else:\n",
    "        print(f\"  âœ… {column_name}: No unseen categories in test data\")\n",
    "    \n",
    "    return train_encoded, test_encoded, le\n",
    "\n",
    "# Apply safe encoding to unit_type_std\n",
    "print(\"\\n1. Encoding unit_type_std...\")\n",
    "train_df['unit_type_encoded'], test_df['unit_type_encoded'], le_unit = safe_label_encode_v2(\n",
    "    train_df['unit_type_std'], \n",
    "    test_df['unit_type_std'],\n",
    "    'unit_type_std'\n",
    ")\n",
    "\n",
    "# Apply safe encoding to category\n",
    "print(\"\\n2. Encoding category...\")\n",
    "train_df['category_encoded'], test_df['category_encoded'], le_category = safe_label_encode_v2(\n",
    "    train_df['category'], \n",
    "    test_df['category'],\n",
    "    'category'\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Categorical encoding completed successfully!\")\n",
    "print(f\"\\nTotal unique unit types encoded: {len(le_unit.classes_)}\")\n",
    "print(f\"Total unique categories encoded: {len(le_category.classes_)}\")\n",
    "\n",
    "# Verify encoding worked\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENCODING VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTrain unit_type_encoded - Min: {train_df['unit_type_encoded'].min()}, Max: {train_df['unit_type_encoded'].max()}\")\n",
    "print(f\"Test unit_type_encoded - Min: {test_df['unit_type_encoded'].min()}, Max: {test_df['unit_type_encoded'].max()}\")\n",
    "print(f\"\\nTrain category_encoded - Min: {train_df['category_encoded'].min()}, Max: {train_df['category_encoded'].max()}\")\n",
    "print(f\"Test category_encoded - Min: {test_df['category_encoded'].min()}, Max: {test_df['category_encoded'].max()}\")\n",
    "\n",
    "print(\"\\nâœ… Encoding ranges are compatible!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FREQUENCY ENCODING FOR ADDITIONAL ROBUSTNESS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FREQUENCY ENCODING (HANDLES UNSEEN VALUES NATURALLY)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def frequency_encoding(train_series, test_series, column_name):\n",
    "    \"\"\"\n",
    "    Encode categories by their frequency in training data\n",
    "    Unseen categories get a default frequency\n",
    "    \"\"\"\n",
    "    # Calculate frequency from training data\n",
    "    freq_map = train_series.value_counts(normalize=True).to_dict()\n",
    "    \n",
    "    # Default frequency for unseen categories (use minimum frequency or small value)\n",
    "    default_freq = min(freq_map.values()) if freq_map else 0.0001\n",
    "    \n",
    "    # Apply encoding\n",
    "    train_encoded = train_series.map(lambda x: freq_map.get(x, default_freq))\n",
    "    test_encoded = test_series.map(lambda x: freq_map.get(x, default_freq))\n",
    "    \n",
    "    # Count unseen\n",
    "    test_set = set(test_series.dropna().unique())\n",
    "    train_set = set(train_series.dropna().unique())\n",
    "    unseen = test_set - train_set\n",
    "    \n",
    "    if unseen:\n",
    "        unseen_count = sum(test_series.isin(unseen))\n",
    "        print(f\"  {column_name}: {len(unseen)} unseen categories ({unseen_count} occurrences) â†’ frequency: {default_freq:.6f}\")\n",
    "    else:\n",
    "        print(f\"  âœ… {column_name}: No unseen categories\")\n",
    "    \n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "# Apply frequency encoding\n",
    "print(\"\\nApplying frequency encoding...\\n\")\n",
    "train_df['unit_type_freq'], test_df['unit_type_freq'] = frequency_encoding(\n",
    "    train_df['unit_type_std'], \n",
    "    test_df['unit_type_std'],\n",
    "    'unit_type_std'\n",
    ")\n",
    "\n",
    "train_df['category_freq'], test_df['category_freq'] = frequency_encoding(\n",
    "    train_df['category'], \n",
    "    test_df['category'],\n",
    "    'category'\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Frequency encoding completed!\")\n",
    "print(f\"\\nFrequency encoding statistics:\")\n",
    "print(f\"  unit_type_freq - Min: {train_df['unit_type_freq'].min():.6f}, Max: {train_df['unit_type_freq'].max():.6f}\")\n",
    "print(f\"  category_freq - Min: {train_df['category_freq'].min():.6f}, Max: {train_df['category_freq'].max():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HANDLE PRICE TRANSFORMATION FOR MODELING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRICE TRANSFORMATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create log-transformed price (helps with skewed distribution)\n",
    "train_df['log_price'] = np.log1p(train_df['price'])\n",
    "\n",
    "# Compare distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(train_df['price'], bins=100, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Original Price Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Price ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(train_df['price'].median(), color='red', linestyle='--', linewidth=2, label=f'Median: ${train_df[\"price\"].median():.2f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(train_df['log_price'], bins=100, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Log-Transformed Price Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Log(Price + 1)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(train_df['log_price'].median(), color='red', linestyle='--', linewidth=2, label=f'Median: {train_df[\"log_price\"].median():.2f}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Distribution Comparison:\")\n",
    "print(f\"  Original Price Skewness: {train_df['price'].skew():.2f}\")\n",
    "print(f\"  Log Price Skewness: {train_df['log_price'].skew():.2f}\")\n",
    "print(f\"\\nâœ… Log transformation reduces skewness by {abs(train_df['price'].skew() - train_df['log_price'].skew()):.2f}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE FINAL FEATURE SET FOR MODELING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL FEATURE SET FOR MODELING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define all engineered features\n",
    "numerical_features = [\n",
    "    'unit_value',\n",
    "    'pack_count',\n",
    "    'total_quantity',\n",
    "    'weight_value',\n",
    "    'num_bullet_points',\n",
    "    'word_count',\n",
    "    'char_count',\n",
    "    'num_numbers',\n",
    "    'avg_word_length'\n",
    "]\n",
    "\n",
    "categorical_features_encoded = [\n",
    "    'unit_type_encoded',\n",
    "    'category_encoded'\n",
    "]\n",
    "\n",
    "frequency_features = [\n",
    "    'unit_type_freq',\n",
    "    'category_freq'\n",
    "]\n",
    "\n",
    "# All features combined\n",
    "all_features = numerical_features + categorical_features_encoded + frequency_features\n",
    "\n",
    "print(f\"ðŸ“Š Total features for modeling: {len(all_features)}\")\n",
    "print(\"\\nFeature breakdown:\")\n",
    "print(f\"  âœ“ Numerical features: {len(numerical_features)}\")\n",
    "print(f\"  âœ“ Encoded categorical: {len(categorical_features_encoded)}\")\n",
    "print(f\"  âœ“ Frequency encoded: {len(frequency_features)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL FEATURES LIST\")\n",
    "print(\"=\" * 80)\n",
    "for i, feat in enumerate(all_features, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# Verify all features exist in both datasets\n",
    "missing_in_train = [f for f in all_features if f not in train_df.columns]\n",
    "missing_in_test = [f for f in all_features if f not in test_df.columns]\n",
    "\n",
    "if missing_in_train:\n",
    "    print(f\"\\nâš ï¸  WARNING: Missing in train: {missing_in_train}\")\n",
    "if missing_in_test:\n",
    "    print(f\"âš ï¸  WARNING: Missing in test: {missing_in_test}\")\n",
    "    \n",
    "if not missing_in_train and not missing_in_test:\n",
    "    print(\"\\nâœ… All features present in both train and test datasets!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE STATISTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTraining data feature statistics:\")\n",
    "display(train_df[all_features].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE CORRELATION WITH PRICE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate correlation with target\n",
    "feature_correlations = train_df[all_features + ['price']].corr()['price'].sort_values(ascending=False)[1:]\n",
    "print(\"\\nTop 10 Features by Correlation with Price:\")\n",
    "print(feature_correlations.head(10))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "feature_correlations.plot(kind='barh', color='teal', edgecolor='black', alpha=0.7)\n",
    "plt.title('Feature Correlations with Price', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Features')\n",
    "plt.axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE PROCESSED DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVE PROCESSED DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save to CSV for later use\n",
    "train_df.to_csv('train_processed.csv', index=False)\n",
    "test_df.to_csv('test_processed.csv', index=False)\n",
    "\n",
    "print(\"âœ… Processed datasets saved!\")\n",
    "print(f\"  ðŸ“ train_processed.csv: {train_df.shape}\")\n",
    "print(f\"  ðŸ“ test_processed.csv: {test_df.shape}\")\n",
    "\n",
    "# Save feature list for later use\n",
    "import json\n",
    "\n",
    "feature_config = {\n",
    "    'all_features': all_features,\n",
    "    'numerical_features': numerical_features,\n",
    "    'categorical_features': categorical_features_encoded,\n",
    "    'frequency_features': frequency_features,\n",
    "    'target': 'price',\n",
    "    'log_target': 'log_price'\n",
    "}\n",
    "\n",
    "with open('feature_config.json', 'w') as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "\n",
    "print(f\"  ðŸ“ feature_config.json: Feature configuration saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ…âœ…âœ… STEP 2 COMPLETED SUCCESSFULLY! âœ…âœ…âœ…\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nðŸ“Š Summary of achievements:\")\n",
    "print(f\"  âœ“ Extracted {len(all_features)} powerful engineered features\")\n",
    "print(f\"  âœ“ Handled {len(le_unit.classes_)} unit types (including unseen values)\")\n",
    "print(f\"  âœ“ Handled {len(le_category.classes_)} product categories\")\n",
    "print(f\"  âœ“ Applied dual encoding strategy (label + frequency)\")\n",
    "print(f\"  âœ“ Log-transformed target reduces skewness from 13.60 â†’ {train_df['log_price'].skew():.2f}\")\n",
    "print(f\"  âœ“ Datasets saved and ready for modeling\")\n",
    "print(f\"\\nðŸŽ¯ Next step: Build baseline and advanced ML models!\")\n",
    "print(f\"   Recommended models: XGBoost, LightGBM, CatBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: BUILD BASELINE & ADVANCED ML MODELS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 3: MODEL BUILDING & TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Import ML libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… ML libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CUSTOM SMAPE METRIC (EVALUATION METRIC)\n",
    "# ============================================================\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "    SMAPE = (100/n) * Î£ |y_pred - y_true| / ((|y_true| + |y_pred|)/2)\n",
    "    \"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    denominator = np.where(denominator == 0, 1e-10, denominator)\n",
    "    \n",
    "    smape = np.mean(numerator / denominator) * 100\n",
    "    return smape\n",
    "\n",
    "# Custom scorer for sklearn cross-validation\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def smape_scorer(y_true, y_pred):\n",
    "    \"\"\"Negative SMAPE for sklearn optimization (sklearn maximizes scores)\"\"\"\n",
    "    return -calculate_smape(y_true, y_pred)\n",
    "\n",
    "smape_score = make_scorer(smape_scorer, greater_is_better=True)\n",
    "\n",
    "print(\"\\nâœ… Custom SMAPE metric defined!\")\n",
    "print(f\"Test SMAPE calculation: {calculate_smape(np.array([100, 200]), np.array([110, 180])):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPARE DATA FOR MODELING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPARING DATA FOR MODELING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load processed data\n",
    "train_df = pd.read_csv('train_processed.csv')\n",
    "test_df = pd.read_csv('test_processed.csv')\n",
    "\n",
    "# Load feature configuration\n",
    "import json\n",
    "with open('feature_config.json', 'r') as f:\n",
    "    feature_config = json.load(f)\n",
    "\n",
    "all_features = feature_config['all_features']\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train = train_df[all_features].copy()\n",
    "y_train = train_df['price'].copy()\n",
    "y_train_log = train_df['log_price'].copy()\n",
    "\n",
    "X_test = test_df[all_features].copy()\n",
    "test_ids = test_df['sample_id'].copy()\n",
    "\n",
    "# Handle any missing values\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "print(f\"âœ… Data prepared!\")\n",
    "print(f\"   Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]:,}\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "print(f\"   Target range: ${y_train.min():.2f} - ${y_train.max():.2f}\")\n",
    "print(f\"   Log target range: {y_train_log.min():.2f} - {y_train_log.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN-VALIDATION SPLIT FOR MODEL EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "y_tr_log, y_val_log = np.log1p(y_tr), np.log1p(y_val)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAIN-VALIDATION SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training set: {X_tr.shape[0]:,} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL 1: BASELINE - SIMPLE GRADIENT BOOSTING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 1: BASELINE GRADIENT BOOSTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "baseline_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training baseline model...\")\n",
    "baseline_model.fit(X_tr, y_tr_log)\n",
    "\n",
    "# Predictions (inverse log transform)\n",
    "baseline_pred_val = np.expm1(baseline_model.predict(X_val))\n",
    "baseline_pred_val = np.maximum(baseline_pred_val, 0.1)  # Ensure positive\n",
    "\n",
    "# Calculate metrics\n",
    "baseline_smape = calculate_smape(y_val, baseline_pred_val)\n",
    "baseline_mae = mean_absolute_error(y_val, baseline_pred_val)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_val, baseline_pred_val))\n",
    "\n",
    "print(f\"\\nâœ… Baseline Model Results:\")\n",
    "print(f\"   SMAPE: {baseline_smape:.4f}%\")\n",
    "print(f\"   MAE: ${baseline_mae:.2f}\")\n",
    "print(f\"   RMSE: ${baseline_rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL 2: XGBOOST (OPTIMIZED)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 2: XGBOOST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 500,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(\n",
    "    X_tr, y_tr_log,\n",
    "    eval_set=[(X_val, y_val_log)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "xgb_pred_val = np.expm1(xgb_model.predict(X_val))\n",
    "xgb_pred_val = np.maximum(xgb_pred_val, 0.1)\n",
    "\n",
    "# Metrics\n",
    "xgb_smape = calculate_smape(y_val, xgb_pred_val)\n",
    "xgb_mae = mean_absolute_error(y_val, xgb_pred_val)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_val, xgb_pred_val))\n",
    "\n",
    "print(f\"\\nâœ… XGBoost Results:\")\n",
    "print(f\"   SMAPE: {xgb_smape:.4f}%\")\n",
    "print(f\"   MAE: ${xgb_mae:.2f}\")\n",
    "print(f\"   RMSE: ${xgb_rmse:.2f}\")\n",
    "print(f\"   Best iteration: {xgb_model.best_iteration}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL 3: LIGHTGBM (FAST & ACCURATE)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 3: LIGHTGBM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 63,\n",
    "    'max_depth': 8,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 500,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\"Training LightGBM model...\")\n",
    "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(\n",
    "    X_tr, y_tr_log,\n",
    "    eval_set=[(X_val, y_val_log)],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "lgb_pred_val = np.expm1(lgb_model.predict(X_val))\n",
    "lgb_pred_val = np.maximum(lgb_pred_val, 0.1)\n",
    "\n",
    "# Metrics\n",
    "lgb_smape = calculate_smape(y_val, lgb_pred_val)\n",
    "lgb_mae = mean_absolute_error(y_val, lgb_pred_val)\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_val, lgb_pred_val))\n",
    "\n",
    "print(f\"\\nâœ… LightGBM Results:\")\n",
    "print(f\"   SMAPE: {lgb_smape:.4f}%\")\n",
    "print(f\"   MAE: ${lgb_mae:.2f}\")\n",
    "print(f\"   RMSE: ${lgb_rmse:.2f}\")\n",
    "print(f\"   Best iteration: {lgb_model.best_iteration_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL 4: CATBOOST (ROBUST & ACCURATE)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 4: CATBOOST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cat_params = {\n",
    "    'iterations': 500,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 8,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'subsample': 0.8,\n",
    "    'random_strength': 1,\n",
    "    'bagging_temperature': 1,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'loss_function': 'RMSE'\n",
    "}\n",
    "\n",
    "print(\"Training CatBoost model...\")\n",
    "cat_model = CatBoostRegressor(**cat_params)\n",
    "cat_model.fit(\n",
    "    X_tr, y_tr_log,\n",
    "    eval_set=(X_val, y_val_log),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "cat_pred_val = np.expm1(cat_model.predict(X_val))\n",
    "cat_pred_val = np.maximum(cat_pred_val, 0.1)\n",
    "\n",
    "# Metrics\n",
    "cat_smape = calculate_smape(y_val, cat_pred_val)\n",
    "cat_mae = mean_absolute_error(y_val, cat_pred_val)\n",
    "cat_rmse = np.sqrt(mean_squared_error(y_val, cat_pred_val))\n",
    "\n",
    "print(f\"\\nâœ… CatBoost Results:\")\n",
    "print(f\"   SMAPE: {cat_smape:.4f}%\")\n",
    "print(f\"   MAE: ${cat_mae:.2f}\")\n",
    "print(f\"   RMSE: ${cat_rmse:.2f}\")\n",
    "print(f\"   Best iteration: {cat_model.get_best_iteration()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Baseline GB', 'XGBoost', 'LightGBM', 'CatBoost'],\n",
    "    'SMAPE (%)': [baseline_smape, xgb_smape, lgb_smape, cat_smape],\n",
    "    'MAE ($)': [baseline_mae, xgb_mae, lgb_mae, cat_mae],\n",
    "    'RMSE ($)': [baseline_rmse, xgb_rmse, lgb_rmse, cat_rmse]\n",
    "})\n",
    "\n",
    "results_df = results_df.sort_values('SMAPE (%)')\n",
    "display(results_df)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# SMAPE comparison\n",
    "axes[0].bar(results_df['Model'], results_df['SMAPE (%)'], color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('SMAPE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('SMAPE (%)')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(results_df['Model'], results_df['MAE ($)'], color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('MAE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('MAE ($)')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# RMSE comparison\n",
    "axes[2].bar(results_df['Model'], results_df['RMSE ($)'], color='green', edgecolor='black', alpha=0.7)\n",
    "axes[2].set_title('RMSE Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('RMSE ($)')\n",
    "axes[2].set_xlabel('Model')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ† Best Model: {results_df.iloc[0]['Model']} (SMAPE: {results_df.iloc[0]['SMAPE (%)']:.4f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: ADVANCED ENSEMBLE & FINAL PREDICTIONS\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SIMPLE WEIGHTED AVERAGE ENSEMBLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WEIGHTED AVERAGE ENSEMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate weights based on inverse SMAPE (better models get higher weights)\n",
    "smape_scores = np.array([xgb_smape, lgb_smape, cat_smape])\n",
    "weights = 1 / smape_scores\n",
    "weights = weights / weights.sum()  # Normalize to sum to 1\n",
    "\n",
    "print(\"Ensemble weights based on SMAPE performance:\")\n",
    "print(f\"  XGBoost:  {weights[0]:.4f} (SMAPE: {xgb_smape:.4f}%)\")\n",
    "print(f\"  LightGBM: {weights[1]:.4f} (SMAPE: {lgb_smape:.4f}%)\")\n",
    "print(f\"  CatBoost: {weights[2]:.4f} (SMAPE: {cat_smape:.4f}%)\")\n",
    "\n",
    "# Create weighted ensemble prediction\n",
    "ensemble_pred_val = (\n",
    "    weights[0] * xgb_pred_val +\n",
    "    weights[1] * lgb_pred_val +\n",
    "    weights[2] * cat_pred_val\n",
    ")\n",
    "\n",
    "# Calculate ensemble SMAPE\n",
    "ensemble_smape = calculate_smape(y_val, ensemble_pred_val)\n",
    "ensemble_mae = mean_absolute_error(y_val, ensemble_pred_val)\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred_val))\n",
    "\n",
    "print(f\"\\nâœ… Weighted Ensemble Results:\")\n",
    "print(f\"   SMAPE: {ensemble_smape:.4f}%\")\n",
    "print(f\"   MAE: ${ensemble_mae:.2f}\")\n",
    "print(f\"   RMSE: ${ensemble_rmse:.2f}\")\n",
    "\n",
    "if ensemble_smape < min(xgb_smape, lgb_smape, cat_smape):\n",
    "    print(f\"\\nðŸŽ‰ Ensemble improved SMAPE by {min(xgb_smape, lgb_smape, cat_smape) - ensemble_smape:.4f}%!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Ensemble SMAPE: {ensemble_smape:.4f}% vs Best Single Model: {min(xgb_smape, lgb_smape, cat_smape):.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# K-FOLD CROSS-VALIDATION FOR ROBUSTNESS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Store CV predictions\n",
    "xgb_cv_scores = []\n",
    "lgb_cv_scores = []\n",
    "cat_cv_scores = []\n",
    "\n",
    "# Store OOF (Out-of-Fold) predictions for ensemble\n",
    "oof_xgb = np.zeros(len(X_train))\n",
    "oof_lgb = np.zeros(len(X_train))\n",
    "oof_cat = np.zeros(len(X_train))\n",
    "\n",
    "# Store test predictions from each fold\n",
    "test_xgb = np.zeros(len(X_test))\n",
    "test_lgb = np.zeros(len(X_test))\n",
    "test_cat = np.zeros(len(X_test))\n",
    "\n",
    "print(\"Training models with 5-fold cross-validation...\\n\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train), 1):\n",
    "    print(f\"Fold {fold}/{n_folds}\")\n",
    "    \n",
    "    X_tr_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    y_tr_log_fold = np.log1p(y_tr_fold)\n",
    "    y_val_log_fold = np.log1p(y_val_fold)\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_fold = xgb.XGBRegressor(**xgb_params)\n",
    "    xgb_fold.fit(X_tr_fold, y_tr_log_fold, eval_set=[(X_val_fold, y_val_log_fold)], verbose=False)\n",
    "    \n",
    "    oof_xgb[val_idx] = np.expm1(xgb_fold.predict(X_val_fold))\n",
    "    test_xgb += np.expm1(xgb_fold.predict(X_test)) / n_folds\n",
    "    \n",
    "    fold_xgb_smape = calculate_smape(y_val_fold, np.maximum(oof_xgb[val_idx], 0.1))\n",
    "    xgb_cv_scores.append(fold_xgb_smape)\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_fold = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_fold.fit(X_tr_fold, y_tr_log_fold, eval_set=[(X_val_fold, y_val_log_fold)], \n",
    "                 callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
    "    \n",
    "    oof_lgb[val_idx] = np.expm1(lgb_fold.predict(X_val_fold))\n",
    "    test_lgb += np.expm1(lgb_fold.predict(X_test)) / n_folds\n",
    "    \n",
    "    fold_lgb_smape = calculate_smape(y_val_fold, np.maximum(oof_lgb[val_idx], 0.1))\n",
    "    lgb_cv_scores.append(fold_lgb_smape)\n",
    "    \n",
    "    # CatBoost\n",
    "    cat_fold = CatBoostRegressor(**cat_params)\n",
    "    cat_fold.fit(X_tr_fold, y_tr_log_fold, eval_set=(X_val_fold, y_val_log_fold), verbose=False)\n",
    "    \n",
    "    oof_cat[val_idx] = np.expm1(cat_fold.predict(X_val_fold))\n",
    "    test_cat += np.expm1(cat_fold.predict(X_test)) / n_folds\n",
    "    \n",
    "    fold_cat_smape = calculate_smape(y_val_fold, np.maximum(oof_cat[val_idx], 0.1))\n",
    "    cat_cv_scores.append(fold_cat_smape)\n",
    "    \n",
    "    print(f\"  XGBoost SMAPE: {fold_xgb_smape:.4f}% | LightGBM: {fold_lgb_smape:.4f}% | CatBoost: {fold_cat_smape:.4f}%\\n\")\n",
    "\n",
    "# Ensure positive predictions\n",
    "oof_xgb = np.maximum(oof_xgb, 0.1)\n",
    "oof_lgb = np.maximum(oof_lgb, 0.1)\n",
    "oof_cat = np.maximum(oof_cat, 0.1)\n",
    "test_xgb = np.maximum(test_xgb, 0.1)\n",
    "test_lgb = np.maximum(test_lgb, 0.1)\n",
    "test_cat = np.maximum(test_cat, 0.1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"XGBoost  - Mean SMAPE: {np.mean(xgb_cv_scores):.4f}% (Â±{np.std(xgb_cv_scores):.4f}%)\")\n",
    "print(f\"LightGBM - Mean SMAPE: {np.mean(lgb_cv_scores):.4f}% (Â±{np.std(lgb_cv_scores):.4f}%)\")\n",
    "print(f\"CatBoost - Mean SMAPE: {np.mean(cat_cv_scores):.4f}% (Â±{np.std(cat_cv_scores):.4f}%)\")\n",
    "\n",
    "# Calculate OOF SMAPE\n",
    "oof_xgb_smape = calculate_smape(y_train, oof_xgb)\n",
    "oof_lgb_smape = calculate_smape(y_train, oof_lgb)\n",
    "oof_cat_smape = calculate_smape(y_train, oof_cat)\n",
    "\n",
    "print(f\"\\nOut-of-Fold SMAPE on entire training set:\")\n",
    "print(f\"  XGBoost:  {oof_xgb_smape:.4f}%\")\n",
    "print(f\"  LightGBM: {oof_lgb_smape:.4f}%\")\n",
    "print(f\"  CatBoost: {oof_cat_smape:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIMIZED ENSEMBLE ON OOF PREDICTIONS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZED WEIGHTED ENSEMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate optimal weights using OOF predictions\n",
    "oof_smape_scores = np.array([oof_xgb_smape, oof_lgb_smape, oof_cat_smape])\n",
    "optimal_weights = 1 / oof_smape_scores\n",
    "optimal_weights = optimal_weights / optimal_weights.sum()\n",
    "\n",
    "print(\"Optimal ensemble weights:\")\n",
    "print(f\"  XGBoost:  {optimal_weights[0]:.4f}\")\n",
    "print(f\"  LightGBM: {optimal_weights[1]:.4f}\")\n",
    "print(f\"  CatBoost: {optimal_weights[2]:.4f}\")\n",
    "\n",
    "# Create ensemble OOF predictions\n",
    "oof_ensemble = (\n",
    "    optimal_weights[0] * oof_xgb +\n",
    "    optimal_weights[1] * oof_lgb +\n",
    "    optimal_weights[2] * oof_cat\n",
    ")\n",
    "\n",
    "oof_ensemble_smape = calculate_smape(y_train, oof_ensemble)\n",
    "print(f\"\\nâœ… Ensemble OOF SMAPE: {oof_ensemble_smape:.4f}%\")\n",
    "\n",
    "# Create ensemble test predictions\n",
    "test_ensemble = (\n",
    "    optimal_weights[0] * test_xgb +\n",
    "    optimal_weights[1] * test_lgb +\n",
    "    optimal_weights[2] * test_cat\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL MODEL COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "final_results = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LightGBM', 'CatBoost', 'Weighted Ensemble'],\n",
    "    'OOF SMAPE (%)': [oof_xgb_smape, oof_lgb_smape, oof_cat_smape, oof_ensemble_smape],\n",
    "    'CV Mean (%)': [np.mean(xgb_cv_scores), np.mean(lgb_cv_scores), np.mean(cat_cv_scores), '-'],\n",
    "    'CV Std (%)': [np.std(xgb_cv_scores), np.std(lgb_cv_scores), np.std(cat_cv_scores), '-']\n",
    "})\n",
    "\n",
    "final_results = final_results.sort_values('OOF SMAPE (%)')\n",
    "display(final_results)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(final_results['Model'], final_results['OOF SMAPE (%)'], \n",
    "        color=['steelblue', 'coral', 'green', 'purple'], \n",
    "        edgecolor='black', alpha=0.7)\n",
    "plt.title('Final Model Comparison - Out-of-Fold SMAPE', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('SMAPE (%)')\n",
    "plt.xlabel('Model')\n",
    "plt.axhline(y=final_results['OOF SMAPE (%)'].min(), color='red', linestyle='--', \n",
    "            linewidth=2, label=f\"Best: {final_results['OOF SMAPE (%)'].min():.4f}%\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ† Best performing model: {final_results.iloc[0]['Model']}\")\n",
    "print(f\"   OOF SMAPE: {final_results.iloc[0]['OOF SMAPE (%)']:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE FINAL SUBMISSION FILE\n",
    "# ============================================================\n",
    "\n",
    "# Use ensemble predictions as final submission\n",
    "final_predictions = test_ensemble\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': test_ids,\n",
    "    'price': final_predictions\n",
    "})\n",
    "\n",
    "# Ensure all predictions are positive\n",
    "submission['price'] = submission['price'].clip(lower=0.1)\n",
    "\n",
    "# Sort by sample_id (required by some competitions)\n",
    "submission = submission.sort_values('sample_id').reset_index(drop=True)\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('test_out.csv', index=False)\n",
    "\n",
    "print(\"âœ… Submission file created: test_out.csv\")\n",
    "print(f\"\\nSubmission file details:\")\n",
    "print(f\"  Total predictions: {len(submission):,}\")\n",
    "print(f\"  Price range: ${submission['price'].min():.2f} - ${submission['price'].max():.2f}\")\n",
    "print(f\"  Mean price: ${submission['price'].mean():.2f}\")\n",
    "print(f\"  Median price: ${submission['price'].median():.2f}\")\n",
    "\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "display(submission.head(10))\n",
    "\n",
    "print(\"\\nLast 10 predictions:\")\n",
    "display(submission.tail(10))\n",
    "\n",
    "# Verify submission format\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUBMISSION FORMAT VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"âœ… Column names: {list(submission.columns)}\")\n",
    "print(f\"âœ… Number of rows: {len(submission)}\")\n",
    "print(f\"âœ… Number of unique sample_ids: {submission['sample_id'].nunique()}\")\n",
    "print(f\"âœ… Any missing values: {submission.isnull().sum().sum()}\")\n",
    "print(f\"âœ… Any negative prices: {(submission['price'] < 0).sum()}\")\n",
    "\n",
    "if len(submission) == 75000 and submission['sample_id'].nunique() == 75000:\n",
    "    print(\"\\nðŸŽ‰ Submission file is VALID and ready for upload!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  WARNING: Check submission file format!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "# Remove early_stopping_rounds for final training (or provide eval_set)\n",
    "xgb_params_final = xgb_params.copy()\n",
    "xgb_params_final.pop('early_stopping_rounds', None)\n",
    "\n",
    "# Get feature importance from XGBoost (best model)\n",
    "xgb_final = xgb.XGBRegressor(**xgb_params_final)\n",
    "xgb_final.fit(X_train, np.log1p(y_train), verbose=False)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'importance': xgb_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "display(feature_importance.head(10))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance['feature'].head(13), \n",
    "         feature_importance['importance'].head(13),\n",
    "         color='teal', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance (XGBoost)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5A: IMAGE DOWNLOAD & PROCESSING SETUP\n",
    "# Amazon ML Challenge 2025 - Add Image Features\n",
    "# ============================================================\n",
    "\n",
    "# Import additional libraries for image processing\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "# Create directories for images\n",
    "os.makedirs('images/train', exist_ok=True)\n",
    "os.makedirs('images/test', exist_ok=True)\n",
    "\n",
    "print(\"âœ… Image processing libraries imported!\")\n",
    "print(\"âœ… Image directories created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMAGE DOWNLOAD UTILITY (BATCH PROCESSING)\n",
    "# ============================================================\n",
    "\n",
    "def download_image(url, save_path, timeout=5, retries=3):\n",
    "    \"\"\"\n",
    "    Download image from URL with retry logic\n",
    "    Returns: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                # Convert to RGB if necessary\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                # Resize to standard size (224x224 for most CNNs)\n",
    "                img = img.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "                img.save(save_path)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                return False\n",
    "            time.sleep(0.5)\n",
    "    return False\n",
    "\n",
    "def batch_download_images(df, split='train', max_workers=32, max_images=None):\n",
    "    \"\"\"\n",
    "    Download images in parallel with progress bar\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DOWNLOADING {split.upper()} IMAGES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Limit images for faster testing\n",
    "    if max_images:\n",
    "        df_subset = df.head(max_images).copy()\n",
    "        print(f\"âš ï¸  Limited to {max_images} images for faster processing\")\n",
    "    else:\n",
    "        df_subset = df.copy()\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    def download_row(row):\n",
    "        sample_id = row['sample_id']\n",
    "        url = row['image_link']\n",
    "        save_path = f'images/{split}/{sample_id}.jpg'\n",
    "        \n",
    "        # Skip if already downloaded\n",
    "        if os.path.exists(save_path):\n",
    "            return True\n",
    "        \n",
    "        return download_image(url, save_path)\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel downloads\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(download_row, [row for _, row in df_subset.iterrows()]),\n",
    "            total=len(df_subset),\n",
    "            desc=f\"Downloading {split} images\"\n",
    "        ))\n",
    "    \n",
    "    successful = sum(results)\n",
    "    failed = len(results) - successful\n",
    "    \n",
    "    print(f\"\\nâœ… Download complete!\")\n",
    "    print(f\"   Successful: {successful:,} images\")\n",
    "    print(f\"   Failed: {failed:,} images\")\n",
    "    print(f\"   Success rate: {successful/len(results)*100:.2f}%\")\n",
    "    \n",
    "    return successful, failed\n",
    "\n",
    "# Download training images (limit to subset for speed)\n",
    "print(\"\\nâ±ï¸  Note: Full download of 75K images takes ~30-60 minutes\")\n",
    "print(\"Starting with 10,000 images for faster testing...\")\n",
    "\n",
    "train_success, train_failed = batch_download_images(\n",
    "    train_df, \n",
    "    split='train', \n",
    "    max_workers=32,\n",
    "    max_images=10000  # Start with 10K for speed; remove for full dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOWNLOAD TEST IMAGES\n",
    "# ============================================================\n",
    "\n",
    "test_success, test_failed = batch_download_images(\n",
    "    test_df, \n",
    "    split='test', \n",
    "    max_workers=32,\n",
    "    max_images=10000  # Start with 10K for speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5B: EXTRACT IMAGE FEATURES USING RESNET50\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IMAGE FEATURE EXTRACTION USING RESNET50\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Import Keras/TensorFlow for feature extraction\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Load pre-trained ResNet50 (without top classification layer)\n",
    "print(\"Loading ResNet50 model...\")\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "print(f\"âœ… ResNet50 loaded! Output shape: {base_model.output_shape}\")\n",
    "\n",
    "def extract_image_features(image_path, model):\n",
    "    \"\"\"\n",
    "    Extract features from a single image using ResNet50\n",
    "    Returns: 2048-dim feature vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        img = keras_image.load_img(image_path, target_size=(224, 224))\n",
    "        img_array = keras_image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        \n",
    "        # Extract features\n",
    "        features = model.predict(img_array, verbose=0)\n",
    "        return features.flatten()\n",
    "    except Exception as e:\n",
    "        # Return zero vector if image fails\n",
    "        return np.zeros(2048)\n",
    "\n",
    "def batch_extract_features(df, split='train', model=None):\n",
    "    \"\"\"\n",
    "    Extract features for all images in dataframe\n",
    "    \"\"\"\n",
    "    print(f\"\\nExtracting features for {split} images...\")\n",
    "    \n",
    "    feature_list = []\n",
    "    sample_ids = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Extracting {split} features\"):\n",
    "        sample_id = row['sample_id']\n",
    "        image_path = f'images/{split}/{sample_id}.jpg'\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            features = extract_image_features(image_path, model)\n",
    "            feature_list.append(features)\n",
    "            sample_ids.append(sample_id)\n",
    "        else:\n",
    "            # Use zero features for missing images\n",
    "            feature_list.append(np.zeros(2048))\n",
    "            sample_ids.append(sample_id)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features_array = np.array(feature_list)\n",
    "    \n",
    "    print(f\"âœ… Extracted features shape: {features_array.shape}\")\n",
    "    return features_array, sample_ids\n",
    "\n",
    "# Extract features from training images\n",
    "train_image_features, train_sample_ids = batch_extract_features(\n",
    "    train_df.head(10000),  # Match downloaded images\n",
    "    split='train',\n",
    "    model=base_model\n",
    ")\n",
    "\n",
    "# Extract features from test images\n",
    "test_image_features, test_sample_ids = batch_extract_features(\n",
    "    test_df.head(10000),  # Match downloaded images\n",
    "    split='test',\n",
    "    model=base_model\n",
    ")\n",
    "\n",
    "# Save features for later use\n",
    "np.save('train_image_features.npy', train_image_features)\n",
    "np.save('test_image_features.npy', test_image_features)\n",
    "np.save('train_image_ids.npy', train_sample_ids)\n",
    "np.save('test_image_ids.npy', test_sample_ids)\n",
    "\n",
    "print(\"\\nâœ… Image features saved to disk!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5C: COMBINE TEXT + IMAGE FEATURES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMBINING TEXT AND IMAGE FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load processed data\n",
    "train_df_full = pd.read_csv('train_processed.csv')\n",
    "test_df_full = pd.read_csv('test_processed.csv')\n",
    "\n",
    "# Get text features (from previous step)\n",
    "text_features = all_features  # 13 text features from Step 2\n",
    "\n",
    "# Load image features\n",
    "train_img_feat = np.load('train_image_features.npy')\n",
    "test_img_feat = np.load('test_image_features.npy')\n",
    "\n",
    "# Create image feature column names\n",
    "img_feature_names = [f'img_feat_{i}' for i in range(train_img_feat.shape[1])]\n",
    "\n",
    "# Add image features to train dataframe (first 10K rows)\n",
    "for i, col_name in enumerate(img_feature_names):\n",
    "    train_df_full.loc[:9999, col_name] = train_img_feat[:, i]\n",
    "\n",
    "# Add image features to test dataframe (first 10K rows)\n",
    "for i, col_name in enumerate(img_feature_names):\n",
    "    test_df_full.loc[:9999, col_name] = test_img_feat[:, i]\n",
    "\n",
    "# Fill missing image features with 0 (for rows without downloaded images)\n",
    "train_df_full[img_feature_names] = train_df_full[img_feature_names].fillna(0)\n",
    "test_df_full[img_feature_names] = test_df_full[img_feature_names].fillna(0)\n",
    "\n",
    "# Combined feature list\n",
    "combined_features = text_features + img_feature_names\n",
    "\n",
    "print(f\"âœ… Combined features created!\")\n",
    "print(f\"   Text features: {len(text_features)}\")\n",
    "print(f\"   Image features: {len(img_feature_names)}\")\n",
    "print(f\"   Total features: {len(combined_features)}\")\n",
    "\n",
    "# Prepare data with combined features\n",
    "X_train_combined = train_df_full.loc[:9999, combined_features].values\n",
    "y_train_combined = train_df_full.loc[:9999, 'price'].values\n",
    "y_train_log_combined = np.log1p(y_train_combined)\n",
    "\n",
    "X_test_combined = test_df_full.loc[:9999, combined_features].values\n",
    "test_ids_combined = test_df_full.loc[:9999, 'sample_id'].values\n",
    "\n",
    "print(f\"\\nCombined training data shape: {X_train_combined.shape}\")\n",
    "print(f\"Combined test data shape: {X_test_combined.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5D: TRAIN IMPROVED MODEL WITH IMAGE FEATURES\n",
    "# (Run this after feature extraction completes)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING IMPROVED MODELS WITH TEXT + IMAGE FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Split data for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr_img, X_val_img, y_tr_img, y_val_img = train_test_split(\n",
    "    X_train_combined, y_train_combined, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "y_tr_log_img = np.log1p(y_tr_img)\n",
    "y_val_log_img = np.log1p(y_val_img)\n",
    "\n",
    "print(f\"Training set: {X_tr_img.shape[0]:,} samples\")\n",
    "print(f\"Validation set: {X_val_img.shape[0]:,} samples\")\n",
    "print(f\"Features: {X_tr_img.shape[1]} (13 text + 2048 image)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN XGBOOST WITH IMAGE FEATURES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"XGBOOST WITH IMAGE FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Optimized parameters for larger feature space\n",
    "xgb_img_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.03,  # Slightly lower for stability\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.5,  # Lower due to many features\n",
    "    'colsample_bylevel': 0.7,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.5,  # Higher regularization for many features\n",
    "    'reg_lambda': 2.0,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 300,\n",
    "    'tree_method': 'gpu_hist',  # Use GPU acceleration!\n",
    "    'gpu_id': 0,\n",
    "    'early_stopping_rounds': 30\n",
    "}\n",
    "\n",
    "print(\"Training XGBoost with GPU acceleration...\")\n",
    "xgb_img_model = xgb.XGBRegressor(**xgb_img_params)\n",
    "xgb_img_model.fit(\n",
    "    X_tr_img, y_tr_log_img,\n",
    "    eval_set=[(X_val_img, y_val_log_img)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "xgb_img_pred_val = np.expm1(xgb_img_model.predict(X_val_img))\n",
    "xgb_img_pred_val = np.maximum(xgb_img_pred_val, 0.1)\n",
    "\n",
    "# Calculate metrics\n",
    "xgb_img_smape = calculate_smape(y_val_img, xgb_img_pred_val)\n",
    "xgb_img_mae = mean_absolute_error(y_val_img, xgb_img_pred_val)\n",
    "xgb_img_rmse = np.sqrt(mean_squared_error(y_val_img, xgb_img_pred_val))\n",
    "\n",
    "print(f\"\\nâœ… XGBoost + Images Results:\")\n",
    "print(f\"   SMAPE: {xgb_img_smape:.4f}%\")\n",
    "print(f\"   MAE: ${xgb_img_mae:.2f}\")\n",
    "print(f\"   RMSE: ${xgb_img_rmse:.2f}\")\n",
    "print(f\"\\nðŸŽ¯ Improvement from text-only: {58.09 - xgb_img_smape:.4f}% reduction in SMAPE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN LIGHTGBM WITH IMAGE FEATURES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LIGHTGBM WITH IMAGE FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "lgb_img_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 50,\n",
    "    'max_depth': 7,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'feature_fraction': 0.7,\n",
    "    'reg_alpha': 0.5,\n",
    "    'reg_lambda': 2.0,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 300,\n",
    "    'device': 'gpu',  # Use GPU\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\"Training LightGBM with GPU acceleration...\")\n",
    "lgb_img_model = lgb.LGBMRegressor(**lgb_img_params)\n",
    "lgb_img_model.fit(\n",
    "    X_tr_img, y_tr_log_img,\n",
    "    eval_set=[(X_val_img, y_val_log_img)],\n",
    "    callbacks=[lgb.early_stopping(30), lgb.log_evaluation(10)]\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "lgb_img_pred_val = np.expm1(lgb_img_model.predict(X_val_img))\n",
    "lgb_img_pred_val = np.maximum(lgb_img_pred_val, 0.1)\n",
    "\n",
    "# Calculate metrics\n",
    "lgb_img_smape = calculate_smape(y_val_img, lgb_img_pred_val)\n",
    "lgb_img_mae = mean_absolute_error(y_val_img, lgb_img_pred_val)\n",
    "lgb_img_rmse = np.sqrt(mean_squared_error(y_val_img, lgb_img_pred_val))\n",
    "\n",
    "print(f\"\\nâœ… LightGBM + Images Results:\")\n",
    "print(f\"   SMAPE: {lgb_img_smape:.4f}%\")\n",
    "print(f\"   MAE: ${lgb_img_mae:.2f}\")\n",
    "print(f\"   RMSE: ${lgb_img_rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARISON: TEXT-ONLY VS TEXT+IMAGE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE COMPARISON: TEXT-ONLY VS TEXT+IMAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'XGBoost (Text Only)',\n",
    "        'XGBoost (Text + Image)',\n",
    "        'LightGBM (Text Only)',\n",
    "        'LightGBM (Text + Image)'\n",
    "    ],\n",
    "    'SMAPE (%)': [\n",
    "        58.09,\n",
    "        xgb_img_smape,\n",
    "        58.33,\n",
    "        lgb_img_smape\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        '-',\n",
    "        f'{58.09 - xgb_img_smape:.2f}%',\n",
    "        '-',\n",
    "        f'{58.33 - lgb_img_smape:.2f}%'\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "# Visualize improvement\n",
    "plt.figure(figsize=(12, 6))\n",
    "models = ['XGBoost\\n(Text Only)', 'XGBoost\\n(Text+Image)', 'LightGBM\\n(Text Only)', 'LightGBM\\n(Text+Image)']\n",
    "scores = [58.09, xgb_img_smape, 58.33, lgb_img_smape]\n",
    "colors = ['lightcoral', 'lightgreen', 'lightcoral', 'lightgreen']\n",
    "\n",
    "bars = plt.bar(models, scores, color=colors, edgecolor='black', alpha=0.7)\n",
    "plt.ylabel('SMAPE (%)', fontsize=12)\n",
    "plt.title('Performance Improvement: Adding Image Features', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=50, color='red', linestyle='--', linewidth=2, label='Target: Top 10 (< 50%)')\n",
    "plt.axhline(y=40, color='green', linestyle='--', linewidth=2, label='Target: Top 3 (< 40%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}%',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_smape = min(xgb_img_smape, lgb_img_smape)\n",
    "print(f\"\\nðŸ† Best Model with Images: {best_smape:.4f}% SMAPE\")\n",
    "print(f\"ðŸ“ˆ Total improvement: {58.09 - best_smape:.4f}%\")\n",
    "\n",
    "if best_smape < 50:\n",
    "    print(f\"\\nðŸŽ‰ EXCELLENT! You're in Top 10-20 range!\")\n",
    "elif best_smape < 55:\n",
    "    print(f\"\\nâœ… GOOD! You're in Top 20-50 range!\")\n",
    "else:\n",
    "    print(f\"\\nðŸ’ª Making progress! Consider more images or fine-tuning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE IMPROVED PREDICTIONS FOR TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING IMPROVED TEST PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use best model for predictions\n",
    "if xgb_img_smape < lgb_img_smape:\n",
    "    best_model_img = xgb_img_model\n",
    "    best_model_name = 'XGBoost + Images'\n",
    "else:\n",
    "    best_model_img = lgb_img_model\n",
    "    best_model_name = 'LightGBM + Images'\n",
    "\n",
    "print(f\"Using {best_model_name} for final predictions...\")\n",
    "\n",
    "# Predict on test set (with image features)\n",
    "test_predictions_img = np.expm1(best_model_img.predict(X_test_combined))\n",
    "test_predictions_img = np.maximum(test_predictions_img, 0.1)\n",
    "\n",
    "# Create improved submission\n",
    "submission_improved = pd.DataFrame({\n",
    "    'sample_id': test_ids_combined,\n",
    "    'price': test_predictions_img\n",
    "})\n",
    "\n",
    "submission_improved = submission_improved.sort_values('sample_id').reset_index(drop=True)\n",
    "submission_improved.to_csv('test_out_improved.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… Improved submission saved: test_out_improved.csv\")\n",
    "print(f\"   Using: {best_model_name}\")\n",
    "print(f\"   Expected SMAPE: ~{best_smape:.2f}%\")\n",
    "print(f\"   Predictions: {len(submission_improved):,} (first 10K with images)\")\n",
    "\n",
    "print(\"\\nâš ï¸  NOTE: This submission covers only first 10K samples\")\n",
    "print(\"For full 75K submission, you need to:\")\n",
    "print(\"1. Download all 75K images (both train + test)\")\n",
    "print(\"2. Extract features for all images\")\n",
    "print(\"3. Retrain on full dataset\")\n",
    "print(\"4. Generate predictions for all 75K test samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 1: DIMENSIONALITY REDUCTION \n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STRATEGY 1: REDUCE IMAGE FEATURE DIMENSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Reduce 2048 image features to 50-100 principal components\n",
    "print(\"Applying PCA to reduce image features from 2048 â†’ 100 dimensions...\")\n",
    "\n",
    "# Standardize image features first\n",
    "scaler = StandardScaler()\n",
    "train_img_scaled = scaler.fit_transform(train_image_features)\n",
    "test_img_scaled = scaler.transform(test_image_features)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=100, random_state=42)\n",
    "train_img_pca = pca.fit_transform(train_img_scaled)\n",
    "test_img_pca = pca.transform(test_img_scaled)\n",
    "\n",
    "print(f\"âœ… Explained variance: {pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "\n",
    "# Create new feature names\n",
    "pca_feature_names = [f'img_pca_{i}' for i in range(100)]\n",
    "\n",
    "# Combine with text features\n",
    "train_df_pca = train_df_full.head(10000).copy()\n",
    "test_df_pca = test_df_full.head(10000).copy()\n",
    "\n",
    "# Add PCA features\n",
    "for i, col_name in enumerate(pca_feature_names):\n",
    "    train_df_pca[col_name] = train_img_pca[:, i]\n",
    "    test_df_pca[col_name] = test_img_pca[:, i]\n",
    "\n",
    "# Combined features: 13 text + 100 PCA image = 113 total\n",
    "reduced_features = text_features + pca_feature_names\n",
    "\n",
    "print(f\"âœ… Reduced features: {len(reduced_features)} (13 text + 100 PCA image)\")\n",
    "\n",
    "# Prepare data\n",
    "X_train_pca = train_df_pca[reduced_features].values\n",
    "y_train_pca = train_df_pca['price'].values\n",
    "y_train_log_pca = np.log1p(y_train_pca)\n",
    "\n",
    "X_test_pca = test_df_pca[reduced_features].values\n",
    "test_ids_pca = test_df_pca['sample_id'].values\n",
    "\n",
    "# Split for validation\n",
    "X_tr_pca, X_val_pca, y_tr_pca, y_val_pca = train_test_split(\n",
    "    X_train_pca, y_train_pca, test_size=0.15, random_state=42\n",
    ")\n",
    "y_tr_log_pca = np.log1p(y_tr_pca)\n",
    "y_val_log_pca = np.log1p(y_val_pca)\n",
    "\n",
    "print(f\"\\nTraining samples: {X_tr_pca.shape[0]:,}\")\n",
    "print(f\"Features: {X_tr_pca.shape[1]} (much better ratio!)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN WITH REDUCED FEATURES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"XGBOOST WITH PCA-REDUCED IMAGE FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "xgb_pca_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 500,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "\n",
    "print(\"Training XGBoost with PCA features...\")\n",
    "xgb_pca_model = xgb.XGBRegressor(**xgb_pca_params)\n",
    "xgb_pca_model.fit(\n",
    "    X_tr_pca, y_tr_log_pca,\n",
    "    eval_set=[(X_val_pca, y_val_log_pca)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "xgb_pca_pred_val = np.expm1(xgb_pca_model.predict(X_val_pca))\n",
    "xgb_pca_pred_val = np.maximum(xgb_pca_pred_val, 0.1)\n",
    "\n",
    "# Calculate metrics\n",
    "xgb_pca_smape = calculate_smape(y_val_pca, xgb_pca_pred_val)\n",
    "xgb_pca_mae = mean_absolute_error(y_val_pca, xgb_pca_pred_val)\n",
    "xgb_pca_rmse = np.sqrt(mean_squared_error(y_val_pca, xgb_pca_pred_val))\n",
    "\n",
    "print(f\"\\nâœ… XGBoost + PCA Images Results:\")\n",
    "print(f\"   SMAPE: {xgb_pca_smape:.4f}%\")\n",
    "print(f\"   MAE: ${xgb_pca_mae:.2f}\")\n",
    "print(f\"   RMSE: ${xgb_pca_rmse:.2f}\")\n",
    "\n",
    "improvement = 58.09 - xgb_pca_smape\n",
    "if improvement > 0:\n",
    "    print(f\"\\nðŸŽ‰ SUCCESS! Improvement: {improvement:.4f}% reduction in SMAPE\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Still worse by {abs(improvement):.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 2: TEXT EMBEDDINGS (FASTER & OFTEN BETTER)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STRATEGY 2: ADD TEXT EMBEDDINGS (FASTER APPROACH)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Install if needed\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"âœ… sentence-transformers already installed\")\n",
    "except:\n",
    "    print(\"Installing sentence-transformers...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', '-q', 'sentence-transformers'])\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load lightweight sentence transformer\n",
    "print(\"\\nLoading sentence transformer model...\")\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"âœ… Model loaded! (384-dim embeddings)\")\n",
    "\n",
    "def extract_text_embeddings(df, text_column='catalog_content'):\n",
    "    \"\"\"Extract text embeddings from catalog content\"\"\"\n",
    "    texts = df[text_column].tolist()\n",
    "    print(f\"Extracting embeddings for {len(texts):,} texts...\")\n",
    "    embeddings = text_model.encode(texts, show_progress_bar=True, batch_size=128)\n",
    "    return embeddings\n",
    "\n",
    "# Extract text embeddings\n",
    "train_text_emb = extract_text_embeddings(train_df_full.head(10000))\n",
    "test_text_emb = extract_text_embeddings(test_df_full.head(10000))\n",
    "\n",
    "# Save embeddings\n",
    "np.save('train_text_embeddings.npy', train_text_emb)\n",
    "np.save('test_text_embeddings.npy', test_text_emb)\n",
    "\n",
    "print(f\"\\nâœ… Text embeddings extracted: {train_text_emb.shape}\")\n",
    "print(f\"   Dimension: 384 (much smaller than 2048 image features!)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMBINE TEXT FEATURES + TEXT EMBEDDINGS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING WITH TEXT FEATURES + TEXT EMBEDDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create text embedding feature names\n",
    "text_emb_names = [f'text_emb_{i}' for i in range(384)]\n",
    "\n",
    "# Add to dataframe\n",
    "train_df_textemb = train_df_full.head(10000).copy()\n",
    "test_df_textemb = test_df_full.head(10000).copy()\n",
    "\n",
    "for i, col_name in enumerate(text_emb_names):\n",
    "    train_df_textemb[col_name] = train_text_emb[:, i]\n",
    "    test_df_textemb[col_name] = test_text_emb[:, i]\n",
    "\n",
    "# Combined: 13 engineered + 384 embeddings = 397 features\n",
    "textemb_features = text_features + text_emb_names\n",
    "\n",
    "print(f\"Combined features: {len(textemb_features)}\")\n",
    "\n",
    "# Prepare data\n",
    "X_train_textemb = train_df_textemb[textemb_features].values\n",
    "y_train_textemb = train_df_textemb['price'].values\n",
    "\n",
    "X_test_textemb = test_df_textemb[textemb_features].values\n",
    "\n",
    "# Split\n",
    "X_tr_te, X_val_te, y_tr_te, y_val_te = train_test_split(\n",
    "    X_train_textemb, y_train_textemb, test_size=0.15, random_state=42\n",
    ")\n",
    "y_tr_log_te = np.log1p(y_tr_te)\n",
    "y_val_log_te = np.log1p(y_val_te)\n",
    "\n",
    "# Train XGBoost\n",
    "print(\"\\nTraining XGBoost with text embeddings...\")\n",
    "xgb_te_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=1.5,\n",
    "    n_estimators=500,\n",
    "    tree_method='gpu_hist',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_te_model.fit(\n",
    "    X_tr_te, y_tr_log_te,\n",
    "    eval_set=[(X_val_te, y_val_log_te)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predict\n",
    "xgb_te_pred = np.expm1(xgb_te_model.predict(X_val_te))\n",
    "xgb_te_pred = np.maximum(xgb_te_pred, 0.1)\n",
    "\n",
    "# Metrics\n",
    "xgb_te_smape = calculate_smape(y_val_te, xgb_te_pred)\n",
    "xgb_te_mae = mean_absolute_error(y_val_te, xgb_te_pred)\n",
    "\n",
    "print(f\"\\nâœ… XGBoost + Text Embeddings Results:\")\n",
    "print(f\"   SMAPE: {xgb_te_smape:.4f}%\")\n",
    "print(f\"   MAE: ${xgb_te_mae:.2f}\")\n",
    "\n",
    "if xgb_te_smape < 58.09:\n",
    "    print(f\"\\nðŸŽ‰ IMPROVED! {58.09 - xgb_te_smape:.4f}% better than text-only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL COMPARISON & DECISION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPLETE PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_complete = pd.DataFrame({\n",
    "    'Approach': [\n",
    "        '1. Text Only (Original)',\n",
    "        '2. Text + Raw Images (2048 feat)',\n",
    "        '3. Text + PCA Images (100 feat)',\n",
    "        '4. Text + Text Embeddings (384 feat)'\n",
    "    ],\n",
    "    'Features': [13, 2061, 113, 397],\n",
    "    'SMAPE (%)': [\n",
    "        58.09,\n",
    "        59.52,\n",
    "        xgb_pca_smape,\n",
    "        xgb_te_smape\n",
    "    ]\n",
    "})\n",
    "\n",
    "results_complete = results_complete.sort_values('SMAPE (%)')\n",
    "display(results_complete)\n",
    "\n",
    "best_approach = results_complete.iloc[0]\n",
    "print(f\"\\nðŸ† BEST APPROACH: {best_approach['Approach']}\")\n",
    "print(f\"   SMAPE: {best_approach['SMAPE (%)']:.4f}%\")\n",
    "print(f\"   Features: {int(best_approach['Features'])}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(results_complete['Approach'], results_complete['SMAPE (%)'], \n",
    "        color=['green' if x == results_complete['SMAPE (%)'].min() else 'coral' \n",
    "               for x in results_complete['SMAPE (%)']],\n",
    "        edgecolor='black', alpha=0.7)\n",
    "plt.ylabel('SMAPE (%)', fontsize=12)\n",
    "plt.title('Performance Comparison: Different Feature Strategies', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=50, color='red', linestyle='--', label='Top 10 Target')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RECOMMENDATION & SUBMISSION STRATEGY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š FINAL RECOMMENDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_smape = min(58.09, xgb_pca_smape, xgb_te_smape)\n",
    "\n",
    "print(\"\\nðŸŽ¯ SUBMISSION STRATEGY:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if best_smape == 58.09:\n",
    "    print(\"âœ… STICK WITH TEXT-ONLY MODEL (58.09% SMAPE)\")\n",
    "    print(\"\\nReason: Adding features made things worse with limited data\")\n",
    "    print(\"\\nRECOMMENDATION:\")\n",
    "    print(\"1. Submit your ORIGINAL text-only submission (test_out.csv)\")\n",
    "    print(\"2. It's your best model with 58.09% SMAPE\")\n",
    "    print(\"3. Expected rank: Top 50-100\")\n",
    "    print(\"\\nTO IMPROVE FURTHER:\")\n",
    "    print(\"- Process full 75K dataset (not just 10K)\")\n",
    "    print(\"- Use text embeddings across all data\")\n",
    "    print(\"- Try ensemble of text-only models with different seeds\")\n",
    "else:\n",
    "    print(f\"âœ… USE IMPROVED MODEL ({best_smape:.4f}% SMAPE)\")\n",
    "    print(\"\\nGenerate new submission with best approach...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"â° TIME CHECK\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Current time: 10:23 AM IST\")\n",
    "print(\"Estimated deadline: 7:00 PM IST (or check competition page)\")\n",
    "print(\"Time remaining: ~8-9 hours\")\n",
    "\n",
    "print(\"\\nðŸ’¡ QUICK WIN STRATEGIES:\")\n",
    "print(\"1. âœ… Submit current best model NOW\")\n",
    "print(\"2. Process full 75K dataset if time permits\")\n",
    "print(\"3. Add text embeddings to full dataset\")\n",
    "print(\"4. Cross-validation ensemble on full data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY: FULL DATASET WITH TEXT EMBEDDINGS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸš€ WINNING STRATEGY: TEXT EMBEDDINGS ON FULL 75K DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load full datasets\n",
    "train_df_full = pd.read_csv('train_processed.csv')\n",
    "test_df_full = pd.read_csv('test_processed.csv')\n",
    "\n",
    "print(f\"Full training data: {len(train_df_full):,} samples\")\n",
    "print(f\"Full test data: {len(test_df_full):,} samples\")\n",
    "\n",
    "# Extract text embeddings for FULL dataset\n",
    "print(\"\\nðŸ”¥ Extracting embeddings for FULL 75K train + 75K test...\")\n",
    "print(\"â±ï¸  This will take ~5-6 minutes...\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load model (already cached)\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract for full train\n",
    "print(\"\\n1. Processing 75K training texts...\")\n",
    "train_text_emb_full = text_model.encode(\n",
    "    train_df_full['catalog_content'].tolist(), \n",
    "    show_progress_bar=True, \n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "# Extract for full test\n",
    "print(\"\\n2. Processing 75K test texts...\")\n",
    "test_text_emb_full = text_model.encode(\n",
    "    test_df_full['catalog_content'].tolist(), \n",
    "    show_progress_bar=True, \n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "# Save\n",
    "np.save('train_text_embeddings_full.npy', train_text_emb_full)\n",
    "np.save('test_text_embeddings_full.npy', test_text_emb_full)\n",
    "\n",
    "print(f\"\\nâœ… Full embeddings extracted!\")\n",
    "print(f\"   Train: {train_text_emb_full.shape}\")\n",
    "print(f\"   Test: {test_text_emb_full.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BUILD FULL DATASET WITH TEXT EMBEDDINGS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMBINING FULL DATA: TEXT FEATURES + EMBEDDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create embedding feature names\n",
    "text_emb_names = [f'text_emb_{i}' for i in range(384)]\n",
    "\n",
    "# Add embeddings to full dataframes\n",
    "for i, col_name in enumerate(text_emb_names):\n",
    "    train_df_full[col_name] = train_text_emb_full[:, i]\n",
    "    test_df_full[col_name] = test_text_emb_full[:, i]\n",
    "\n",
    "# Combined features: 13 + 384 = 397\n",
    "full_features = text_features + text_emb_names\n",
    "\n",
    "print(f\"âœ… Combined features: {len(full_features)}\")\n",
    "\n",
    "# Prepare full training data\n",
    "X_train_full = train_df_full[full_features].values\n",
    "y_train_full = train_df_full['price'].values\n",
    "y_train_log_full = np.log1p(y_train_full)\n",
    "\n",
    "X_test_full = test_df_full[full_features].values\n",
    "test_ids_full = test_df_full['sample_id'].values\n",
    "\n",
    "print(f\"\\nðŸ“Š Full dataset prepared:\")\n",
    "print(f\"   Training: {X_train_full.shape}\")\n",
    "print(f\"   Test: {X_test_full.shape}\")\n",
    "print(f\"   Feature-to-sample ratio: 1:{X_train_full.shape[0]//X_train_full.shape[1]}\")\n",
    "print(f\"   âœ… Much better ratio for learning!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN FINAL MODEL ON FULL 75K DATASET\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ† TRAINING FINAL MODEL ON FULL 75K DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Split for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr_full, X_val_full, y_tr_full, y_val_full = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "y_tr_log_full = np.log1p(y_tr_full)\n",
    "y_val_log_full = np.log1p(y_val_full)\n",
    "\n",
    "print(f\"Training: {X_tr_full.shape[0]:,} samples\")\n",
    "print(f\"Validation: {X_val_full.shape[0]:,} samples\")\n",
    "\n",
    "# Optimized parameters for full dataset\n",
    "xgb_full_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.2,\n",
    "    'reg_lambda': 1.5,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 500,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "\n",
    "print(\"\\nðŸš€ Training XGBoost on full 75K dataset with GPU...\")\n",
    "xgb_full_model = xgb.XGBRegressor(**xgb_full_params)\n",
    "\n",
    "xgb_full_model.fit(\n",
    "    X_tr_full, y_tr_log_full,\n",
    "    eval_set=[(X_val_full, y_val_log_full)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "xgb_full_pred_val = np.expm1(xgb_full_model.predict(X_val_full))\n",
    "xgb_full_pred_val = np.maximum(xgb_full_pred_val, 0.1)\n",
    "\n",
    "# Metrics\n",
    "xgb_full_smape = calculate_smape(y_val_full, xgb_full_pred_val)\n",
    "xgb_full_mae = mean_absolute_error(y_val_full, xgb_full_pred_val)\n",
    "xgb_full_rmse = np.sqrt(mean_squared_error(y_val_full, xgb_full_pred_val))\n",
    "\n",
    "print(f\"\\nâœ… FULL DATASET RESULTS:\")\n",
    "print(f\"   SMAPE: {xgb_full_smape:.4f}%\")\n",
    "print(f\"   MAE: ${xgb_full_mae:.2f}\")\n",
    "print(f\"   RMSE: ${xgb_full_rmse:.2f}\")\n",
    "\n",
    "improvement = 58.09 - xgb_full_smape\n",
    "if improvement > 0:\n",
    "    print(f\"\\nðŸŽ‰ MAJOR IMPROVEMENT: {improvement:.4f}% better than text-only!\")\n",
    "    print(f\"   This is what we needed! Full data makes embeddings work!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Still similar to text-only: {xgb_full_smape:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SUBMISSION - FULL 75K TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“ GENERATING FINAL SUBMISSION - FULL 75K\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Predict on full test set\n",
    "final_predictions = np.expm1(xgb_full_model.predict(X_test_full))\n",
    "final_predictions = np.maximum(final_predictions, 0.1)\n",
    "\n",
    "# Create submission\n",
    "final_submission = pd.DataFrame({\n",
    "    'sample_id': test_ids_full,\n",
    "    'price': final_predictions\n",
    "})\n",
    "\n",
    "final_submission = final_submission.sort_values('sample_id').reset_index(drop=True)\n",
    "final_submission.to_csv('test_out_FINAL.csv', index=False)\n",
    "\n",
    "print(f\"âœ… FINAL submission created: test_out_FINAL.csv\")\n",
    "print(f\"   Samples: {len(final_submission):,}\")\n",
    "print(f\"   Expected SMAPE: ~{xgb_full_smape:.2f}%\")\n",
    "print(f\"   Model: XGBoost + Text Embeddings (75K data)\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUBMISSION VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"âœ… Total rows: {len(final_submission)}\")\n",
    "print(f\"âœ… Unique sample_ids: {final_submission['sample_id'].nunique()}\")\n",
    "print(f\"âœ… Missing values: {final_submission.isnull().sum().sum()}\")\n",
    "print(f\"âœ… Price range: ${final_submission['price'].min():.2f} - ${final_submission['price'].max():.2f}\")\n",
    "\n",
    "if len(final_submission) == 75000 and final_submission['sample_id'].nunique() == 75000:\n",
    "    print(\"\\nðŸŽ‰ PERFECT! Submission is ready for upload!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Check submission format!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL DECISION MATRIX\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š COMPLETE DECISION MATRIX\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Text-Only (13 feat) - Full 75K',\n",
    "        'Text+Embeddings (397 feat) - Full 75K'\n",
    "    ],\n",
    "    'Training Data': [\n",
    "        '75,000',\n",
    "        '75,000'\n",
    "    ],\n",
    "    'Features': [13, 397],\n",
    "    'SMAPE (%)': [\n",
    "        58.09,  # Your original best\n",
    "        xgb_full_smape  # New full dataset result\n",
    "    ],\n",
    "    'File': [\n",
    "        'test_out.csv (original)',\n",
    "        'test_out_FINAL.csv (new)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(final_comparison)\n",
    "\n",
    "# Decision logic\n",
    "best_idx = final_comparison['SMAPE (%)'].idxmin()\n",
    "best_model = final_comparison.loc[best_idx]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model['Model']}\")\n",
    "print(f\"SMAPE: {best_model['SMAPE (%)']:.4f}%\")\n",
    "print(f\"Submit: {best_model['File']}\")\n",
    "\n",
    "if xgb_full_smape < 58.09:\n",
    "    print(\"\\nâœ… RECOMMENDATION: Submit test_out_FINAL.csv\")\n",
    "    print(\"   Text embeddings work well with full 75K data!\")\n",
    "    print(f\"   Improvement: {58.09 - xgb_full_smape:.4f}%\")\n",
    "else:\n",
    "    print(\"\\nâœ… RECOMMENDATION: Submit test_out.csv (original)\")\n",
    "    print(\"   Original text-only ensemble is still best\")\n",
    "    print(\"   (Text embeddings didn't improve enough)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 1: ENSEMBLE WITH MULTIPLE RANDOM SEEDS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENSEMBLE STRATEGY: MULTIPLE RANDOM SEEDS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Train 5 models with different random seeds\n",
    "n_models = 5\n",
    "models = []\n",
    "predictions_val = []\n",
    "predictions_test = []\n",
    "\n",
    "for seed in [42, 123, 456, 789, 2024]:\n",
    "    print(f\"\\nTraining model with seed {seed}...\")\n",
    "    \n",
    "    # Split with different seed\n",
    "    X_tr_seed, X_val_seed, y_tr_seed, y_val_seed = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.15, random_state=seed\n",
    "    )\n",
    "    y_tr_log_seed = np.log1p(y_tr_seed)\n",
    "    \n",
    "    # Train model\n",
    "    model_seed = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.7,\n",
    "        reg_alpha=0.2,\n",
    "        reg_lambda=1.5,\n",
    "        random_state=seed,\n",
    "        n_estimators=500,\n",
    "        tree_method='gpu_hist',\n",
    "        gpu_id=0\n",
    "    )\n",
    "    \n",
    "    model_seed.fit(X_tr_seed, y_tr_log_seed, verbose=False)\n",
    "    \n",
    "    # Predict\n",
    "    pred_val = np.expm1(model_seed.predict(X_val_seed))\n",
    "    pred_test = np.expm1(model_seed.predict(X_test_full))\n",
    "    \n",
    "    predictions_val.append((pred_val, y_val_seed))\n",
    "    predictions_test.append(pred_test)\n",
    "    models.append(model_seed)\n",
    "    \n",
    "    # Individual model SMAPE\n",
    "    smape_seed = calculate_smape(y_val_seed, np.maximum(pred_val, 0.1))\n",
    "    print(f\"   Seed {seed} SMAPE: {smape_seed:.4f}%\")\n",
    "\n",
    "# Average ensemble\n",
    "print(\"\\nCreating ensemble average...\")\n",
    "ensemble_test = np.mean(predictions_test, axis=0)\n",
    "ensemble_test = np.maximum(ensemble_test, 0.1)\n",
    "\n",
    "# Calculate ensemble validation SMAPE (approximate)\n",
    "print(\"âœ… Ensemble created with 5 models!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIX: CATBOOST PARAMETERS (REMOVE SUBSAMPLE)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n2. Training CatBoost on full data (FIXED)...\")\n",
    "cat_full_params = {\n",
    "    'iterations': 500,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 7,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'loss_function': 'RMSE',\n",
    "    'task_type': 'GPU',\n",
    "    'bootstrap_type': 'Bernoulli',  # Required for subsample\n",
    "    'subsample': 0.8  # Now it will work\n",
    "}\n",
    "\n",
    "cat_full_model = CatBoostRegressor(**cat_full_params)\n",
    "cat_full_model.fit(\n",
    "    X_tr_full, y_tr_log_full,\n",
    "    eval_set=(X_val_full, y_val_log_full),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_pred_val = np.expm1(cat_full_model.predict(X_val_full))\n",
    "cat_pred_test = np.expm1(cat_full_model.predict(X_test_full))\n",
    "\n",
    "cat_smape = calculate_smape(y_val_full, np.maximum(cat_pred_val, 0.1))\n",
    "print(f\"   CatBoost SMAPE: {cat_smape:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIMIZED WEIGHTED ENSEMBLE\n",
    "# =======================================\n",
    "# Collect all predictions\n",
    "all_preds_val = [xgb_full_pred_val, lgb_pred_val, cat_pred_val]\n",
    "all_preds_test = [\n",
    "    np.expm1(xgb_full_model.predict(X_test_full)),\n",
    "    lgb_pred_test,\n",
    "    cat_pred_test\n",
    "]\n",
    "\n",
    "# Model SMAPEs\n",
    "model_smapes = {\n",
    "    'XGBoost': xgb_full_smape,  # 55.57%\n",
    "    'LightGBM': 56.24,  # From your output\n",
    "    'CatBoost': cat_smape  # Will be calculated\n",
    "}\n",
    "\n",
    "print(\"\\nIndividual Model Performance:\")\n",
    "for name, smape in model_smapes.items():\n",
    "    print(f\"  {name}: {smape:.4f}%\")\n",
    "\n",
    "# Calculate weights based on inverse SMAPE\n",
    "smapes = np.array([xgb_full_smape, 56.24, cat_smape])\n",
    "weights_optimal = 1 / smapes\n",
    "weights_optimal = weights_optimal / weights_optimal.sum()\n",
    "\n",
    "print(\"\\nOptimal weights:\")\n",
    "print(f\"  XGBoost:  {weights_optimal[0]:.4f}\")\n",
    "print(f\"  LightGBM: {weights_optimal[1]:.4f}\")\n",
    "print(f\"  CatBoost: {weights_optimal[2]:.4f}\")\n",
    "\n",
    "# Create weighted ensemble\n",
    "final_ensemble_val = sum(w * p for w, p in zip(weights_optimal, all_preds_val))\n",
    "final_ensemble_test = sum(w * p for w, p in zip(weights_optimal, all_preds_test))\n",
    "\n",
    "final_ensemble_val = np.maximum(final_ensemble_val, 0.1)\n",
    "final_ensemble_test = np.maximum(final_ensemble_test, 0.1)\n",
    "\n",
    "# Calculate ensemble SMAPE\n",
    "ensemble_smape = calculate_smape(y_val_full, final_ensemble_val)\n",
    "print(f\"\\nâœ… Weighted Ensemble SMAPE: {ensemble_smape:.4f}%\")\n",
    "\n",
    "improvement = xgb_full_smape - ensemble_smape\n",
    "if improvement > 0:\n",
    "    print(f\"ðŸŽ‰ IMPROVED! {improvement:.4f}% better than single XGBoost!\")\n",
    "else:\n",
    "    print(f\"âš ï¸  XGBoost alone still best ({xgb_full_smape:.4f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "multi_seed_test = np.mean(predictions_test, axis=0)\n",
    "multi_seed_test = np.maximum(multi_seed_test, 0.1)\n",
    "\n",
    "multi_seed_val_smapes = [53.79, 53.72, 54.14, 53.94, 54.46]  # From output\n",
    "avg_multi_seed_smape = np.mean(multi_seed_val_smapes)\n",
    "\n",
    "print(f\"\\nMulti-seed ensemble average SMAPE: {avg_multi_seed_smapes:.4f}%\")\n",
    "print(f\"Best single seed: {min(multi_seed_val_smapes):.4f}%\")\n",
    "print(f\"Worst single seed: {max(multi_seed_val_smapes):.4f}%\")\n",
    "print(f\"Standard deviation: {np.std(multi_seed_val_smapes):.4f}%\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Multi-seed ensemble reduces variance!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compare all approaches\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Approach': [\n",
    "        'Single XGBoost (seed=42)',\n",
    "        'Multi-seed XGBoost (5 models)',\n",
    "        'XGB+LGB+CAT Weighted Ensemble'\n",
    "    ],\n",
    "    'Validation SMAPE (%)': [\n",
    "        55.57,\n",
    "        avg_multi_seed_smape,\n",
    "        ensemble_smape\n",
    "    ]\n",
    "})\n",
    "\n",
    "final_comparison = final_comparison.sort_values('Validation SMAPE (%)')\n",
    "display(final_comparison)\n",
    "\n",
    "# Select best approach\n",
    "best_approach = final_comparison.iloc[0]\n",
    "best_final_smape = best_approach['Validation SMAPE (%)']\n",
    "\n",
    "print(f\"\\nðŸŽ¯ BEST APPROACH: {best_approach['Approach']}\")\n",
    "print(f\"   Validation SMAPE: {best_final_smape:.4f}%\")\n",
    "\n",
    "# Determine which predictions to use\n",
    "if best_approach['Approach'] == 'Multi-seed XGBoost (5 models)':\n",
    "    final_best_predictions = multi_seed_test\n",
    "    final_method = \"Multi-seed XGBoost Ensemble\"\n",
    "elif best_approach['Approach'] == 'XGB+LGB+CAT Weighted Ensemble':\n",
    "    final_best_predictions = final_ensemble_test\n",
    "    final_method = \"Weighted Multi-Algorithm Ensemble\"\n",
    "else:\n",
    "    final_best_predictions = np.expm1(xgb_full_model.predict(X_test_full))\n",
    "    final_best_predictions = np.maximum(final_best_predictions, 0.1)\n",
    "    final_method = \"Single XGBoost\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL ULTRA-OPTIMIZED SUBMISSION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“ GENERATING ULTRA-OPTIMIZED SUBMISSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create final submission\n",
    "ultra_submission = pd.DataFrame({\n",
    "    'sample_id': test_ids_full,\n",
    "    'price': final_best_predictions\n",
    "})\n",
    "\n",
    "ultra_submission = ultra_submission.sort_values('sample_id').reset_index(drop=True)\n",
    "ultra_submission.to_csv('test_out_ULTRA.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Ultra-optimized submission created!\")\n",
    "print(f\"   File: test_out_ULTRA.csv\")\n",
    "print(f\"   Method: {final_method}\")\n",
    "print(f\"   Validation SMAPE: {best_final_smape:.4f}%\")\n",
    "print(f\"   Total samples: {len(ultra_submission):,}\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"âœ… Rows: {len(ultra_submission)} (expected: 75,000)\")\n",
    "print(f\"âœ… Unique IDs: {ultra_submission['sample_id'].nunique()} (expected: 75,000)\")\n",
    "print(f\"âœ… Missing values: {ultra_submission.isnull().sum().sum()}\")\n",
    "print(f\"âœ… Price range: ${ultra_submission['price'].min():.2f} - ${ultra_submission['price'].max():.2f}\")\n",
    "\n",
    "if len(ultra_submission) == 75000 and ultra_submission['sample_id'].nunique() == 75000:\n",
    "    print(\"\\nðŸŽ‰ PERFECT! Ready for submission!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PERFORMANCE SUMMARY & RANK ESTIMATION\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Stage': [\n",
    "        'Baseline (Text-Only)',\n",
    "        'Text + Embeddings',\n",
    "        'Optimized Ensemble'\n",
    "    ],\n",
    "    'SMAPE (%)': [\n",
    "        58.09,\n",
    "        55.57,\n",
    "        best_final_smape\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        'â€”',\n",
    "        f'-{58.09-55.57:.2f}%',\n",
    "        f'-{58.09-best_final_smape:.2f}%'\n",
    "    ],\n",
    "    'File': [\n",
    "        'test_out.csv',\n",
    "        'test_out_FINAL.csv',\n",
    "        'test_out_ULTRA.csv'\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(summary_df)\n",
    "\n",
    "total_improvement = 58.09 - best_final_smape\n",
    "relative_improvement = (total_improvement / 58.09) * 100\n",
    "\n",
    "print(f\"\\nðŸŽ¯ TOTAL PROGRESS:\")\n",
    "print(f\"   Starting SMAPE: 58.09%\")\n",
    "print(f\"   Final SMAPE: {best_final_smape:.4f}%\")\n",
    "print(f\"   Absolute improvement: {total_improvement:.2f}%\")\n",
    "print(f\"   Relative improvement: {relative_improvement:.1f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Create submission folder\n",
    "submission_folder = f'amazon_ml_submission_{datetime.now().strftime(\"%Y%m%d_%H%M\")}'\n",
    "os.makedirs(submission_folder, exist_ok=True)\n",
    "\n",
    "# Copy files\n",
    "shutil.copy('test_out_ULTRA.csv', f'{submission_folder}/test_out.csv')\n",
    "shutil.copy('FINAL_SUBMISSION_DOCUMENTATION.txt', f'{submission_folder}/approach_documentation.txt')\n",
    "\n",
    "print(f\"âœ… Submission package created: {submission_folder}/\")\n",
    "print(\"\\nPackage contents:\")\n",
    "print(\"  1. test_out.csv (75,000 predictions)\")\n",
    "print(\"  2. approach_documentation.txt (methodology)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPROVEMENT 1: DOWNLOAD & PROCESS ALL 75K IMAGES\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Create image directories\n",
    "os.makedirs('images/train_full', exist_ok=True)\n",
    "os.makedirs('images/test_full', exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: DOWNLOAD ALL 75K TRAIN + 75K TEST IMAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def download_image_fast(url, save_path, timeout=5, retries=2):\n",
    "    \"\"\"Fast image download with minimal retries\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                img = img.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "                img.save(save_path, quality=85, optimize=True)\n",
    "                return True\n",
    "        except:\n",
    "            if attempt == retries - 1:\n",
    "                return False\n",
    "            time.sleep(0.3)\n",
    "    return False\n",
    "\n",
    "def batch_download_images_parallel(df, split='train_full', max_workers=64):\n",
    "    \"\"\"Download images with high parallelization\"\"\"\n",
    "    print(f\"\\nðŸ“¥ Downloading {len(df):,} {split} images...\")\n",
    "    print(f\"   Using {max_workers} parallel workers...\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    def download_row(row):\n",
    "        sample_id = row['sample_id']\n",
    "        url = row['image_link']\n",
    "        save_path = f'images/{split}/{sample_id}.jpg'\n",
    "        \n",
    "        if os.path.exists(save_path):\n",
    "            return True\n",
    "        \n",
    "        return download_image_fast(url, save_path)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(download_row, [row for _, row in df.iterrows()]),\n",
    "            total=len(df),\n",
    "            desc=f\"Downloading {split}\"\n",
    "        ))\n",
    "    \n",
    "    successful = sum(results)\n",
    "    failed = len(results) - successful\n",
    "    \n",
    "    print(f\"âœ… {split.upper()} download complete!\")\n",
    "    print(f\"   Successful: {successful:,} ({successful/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Failed: {failed:,}\")\n",
    "    \n",
    "    return successful, failed\n",
    "\n",
    "# Download training images (FULL 75K)\n",
    "print(\"\\nâ±ï¸  Estimated time: ~10-15 minutes for 75K images\")\n",
    "train_success, train_failed = batch_download_images_parallel(\n",
    "    train_df_full, \n",
    "    split='train_full', \n",
    "    max_workers=64\n",
    ")\n",
    "\n",
    "# Download test images (FULL 75K)\n",
    "test_success, test_failed = batch_download_images_parallel(\n",
    "    test_df_full, \n",
    "    split='test_full', \n",
    "    max_workers=64\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… TOTAL IMAGES DOWNLOADED:\")\n",
    "print(f\"   Train: {train_success:,} / 75,000 ({train_success/75000*100:.1f}%)\")\n",
    "print(f\"   Test: {test_success:,} / 75,000 ({test_success/75000*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Force TensorFlow to use CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Restart Python kernel first, then run:\n",
    "print(\"âœ… TensorFlow will now use CPU\")\n",
    "print(\"â±ï¸  Expected time: ~20-25 minutes for 75K images (slower but stable)\")\n",
    "\n",
    "# NOW reload ResNet50\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "print(f\"âœ… ResNet50 loaded on CPU! Output: {base_model.output_shape[1]} features\")\n",
    "\n",
    "# Use the SAME extraction code, it will work on CPU\n",
    "def extract_features_batch(df, split='train_full', model=None, batch_size=32):  # Smaller batch for CPU\n",
    "    \"\"\"Extract features in batches (CPU-optimized)\"\"\"\n",
    "    print(f\"\\nExtracting features from {len(df):,} images (CPU mode)...\")\n",
    "    \n",
    "    feature_list = []\n",
    "    sample_ids = []\n",
    "    \n",
    "    n_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx in tqdm(range(n_batches), desc=f\"Extracting {split}\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        batch_images = []\n",
    "        batch_ids = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            sample_id = row['sample_id']\n",
    "            image_path = f'images/{split}/{sample_id}.jpg'\n",
    "            \n",
    "            if os.path.exists(image_path):\n",
    "                try:\n",
    "                    img = keras_image.load_img(image_path, target_size=(224, 224))\n",
    "                    img_array = keras_image.img_to_array(img)\n",
    "                    batch_images.append(img_array)\n",
    "                    batch_ids.append(sample_id)\n",
    "                except:\n",
    "                    batch_images.append(np.zeros((224, 224, 3)))\n",
    "                    batch_ids.append(sample_id)\n",
    "            else:\n",
    "                batch_images.append(np.zeros((224, 224, 3)))\n",
    "                batch_ids.append(sample_id)\n",
    "        \n",
    "        if batch_images:\n",
    "            batch_array = np.array(batch_images)\n",
    "            batch_array = preprocess_input(batch_array)\n",
    "            features = model.predict(batch_array, verbose=0)\n",
    "            feature_list.extend(features)\n",
    "            sample_ids.extend(batch_ids)\n",
    "    \n",
    "    features_array = np.array(feature_list)\n",
    "    print(f\"âœ… Extracted: {features_array.shape}\")\n",
    "    return features_array, sample_ids\n",
    "\n",
    "# Extract features\n",
    "train_img_feat_full, train_img_ids = extract_features_batch(\n",
    "    train_df_full,\n",
    "    split='train_full',\n",
    "    model=base_model,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "test_img_feat_full, test_img_ids = extract_features_batch(\n",
    "    test_df_full,\n",
    "    split='test_full',\n",
    "    model=base_model,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Save\n",
    "np.save('train_image_features_75k.npy', train_img_feat_full)\n",
    "np.save('test_image_features_75k.npy', test_img_feat_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALTERNATIVE: USE EFFICIENTNET-LITE (LIGHTER & FASTER)\n",
    "# =========\n",
    "import subprocess\n",
    "subprocess.check_call(['pip', 'install', '-q', 'efficientnet'])\n",
    "\n",
    "import efficientnet.keras as efn\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# Load EfficientNetB0 (lighter, faster, fewer features)\n",
    "base_model = efn.EfficientNetB0(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "print(f\"âœ… EfficientNetB0 loaded! Output: {base_model.output_shape[1]} features (1280)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLIP (NO TENSORFLOW NEEDED)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# Install CLIP\n",
    "import subprocess\n",
    "subprocess.check_call(['pip', 'install', '-q', 'git+https://github.com/openai/CLIP.git'])\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(f\"âœ… CLIP loaded on {device}! Output: 512 features\")\n",
    "\n",
    "def extract_clip_features(df, split='train_full'):\n",
    "    \"\"\"Extract CLIP features (FAST)\"\"\"\n",
    "    print(f\"\\nExtracting CLIP features from {len(df):,} images...\")\n",
    "    \n",
    "    features_list = []\n",
    "    ids_list = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"CLIP {split}\"):\n",
    "        sample_id = row['sample_id']\n",
    "        image_path = f'images/{split}/{sample_id}.jpg'\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(image_path):\n",
    "                image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    features = model.encode_image(image)\n",
    "                    features = features.cpu().numpy().flatten()\n",
    "            else:\n",
    "                features = np.zeros(512)\n",
    "        except:\n",
    "            features = np.zeros(512)\n",
    "        \n",
    "        features_list.append(features)\n",
    "        ids_list.append(sample_id)\n",
    "    \n",
    "    return np.array(features_list), ids_list\n",
    "\n",
    "# Extract features\n",
    "train_img_feat_full, train_img_ids = extract_clip_features(train_df_full, 'train_full')\n",
    "test_img_feat_full, test_img_ids = extract_clip_features(test_df_full, 'test_full')\n",
    "\n",
    "# Save\n",
    "np.save('train_image_features_clip_75k.npy', train_img_feat_full)\n",
    "np.save('test_image_features_clip_75k.npy', test_img_feat_full)\n",
    "\n",
    "print(f\"\\nâœ… CLIP features extracted!\")\n",
    "print(f\"   Shape: {train_img_feat_full.shape} (512 dims instead of 2048)\")\n",
    "print(f\"   Time saved: ~50% faster than ResNet50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: DIMENSIONALITY REDUCTION WITH PCA\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features\n",
    "print(\"Standardizing image features...\")\n",
    "scaler_img = StandardScaler()\n",
    "train_img_scaled = scaler_img.fit_transform(train_img_feat_full)\n",
    "test_img_scaled = scaler_img.transform(test_img_feat_full)\n",
    "\n",
    "# Apply PCA (keep 100 components for good balance)\n",
    "print(\"Applying PCA (2048 â†’ 100 dimensions)...\")\n",
    "pca_img = PCA(n_components=100, random_state=42)\n",
    "train_img_pca_full = pca_img.fit_transform(train_img_scaled)\n",
    "test_img_pca_full = pca_img.transform(test_img_scaled)\n",
    "\n",
    "explained_var = pca_img.explained_variance_ratio_.sum()\n",
    "print(f\"âœ… PCA complete!\")\n",
    "print(f\"   Explained variance: {explained_var*100:.2f}%\")\n",
    "print(f\"   Shape: {train_img_pca_full.shape}\")\n",
    "\n",
    "# Save PCA features\n",
    "np.save('train_image_pca_75k.npy', train_img_pca_full)\n",
    "np.save('test_image_pca_75k.npy', test_img_pca_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: COMBINE TEXT + TEXT EMBEDDINGS + IMAGE PCA\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# Load existing features\n",
    "train_text_emb_full = np.load('train_text_embeddings_full.npy')\n",
    "test_text_emb_full = np.load('test_text_embeddings_full.npy')\n",
    "\n",
    "# Create comprehensive feature set\n",
    "# 13 text features + 384 text embeddings + 100 image PCA = 497 features\n",
    "\n",
    "# Add image PCA to dataframes\n",
    "img_pca_names = [f'img_pca_{i}' for i in range(100)]\n",
    "\n",
    "for i, col_name in enumerate(img_pca_names):\n",
    "    train_df_full[col_name] = train_img_pca_full[:, i]\n",
    "    test_df_full[col_name] = test_img_pca_full[:, i]\n",
    "\n",
    "# Combined features\n",
    "ultimate_features = text_features + text_emb_names + img_pca_names\n",
    "\n",
    "print(f\"âœ… Ultimate feature set created!\")\n",
    "print(f\"   Text features: {len(text_features)}\")\n",
    "print(f\"   Text embeddings: {len(text_emb_names)}\")\n",
    "print(f\"   Image PCA: {len(img_pca_names)}\")\n",
    "print(f\"   TOTAL: {len(ultimate_features)} features\")\n",
    "\n",
    "# Prepare data\n",
    "X_train_ultimate = train_df_full[ultimate_features].values\n",
    "y_train_ultimate = train_df_full['price'].values\n",
    "y_train_log_ultimate = np.log1p(y_train_ultimate)\n",
    "\n",
    "X_test_ultimate = test_df_full[ultimate_features].values\n",
    "test_ids_ultimate = test_df_full['sample_id'].values\n",
    "\n",
    "print(f\"\\nðŸ“Š Final dataset:\")\n",
    "print(f\"   Training: {X_train_ultimate.shape}\")\n",
    "print(f\"   Test: {X_test_ultimate.shape}\")\n",
    "print(f\"   Feature-to-sample ratio: 1:{X_train_ultimate.shape[0]//X_train_ultimate.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5: TRAIN ULTIMATE MODEL WITH ALL FEATURES\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split\n",
    "X_tr_ult, X_val_ult, y_tr_ult, y_val_ult = train_test_split(\n",
    "    X_train_ultimate, y_train_ultimate, test_size=0.15, random_state=42\n",
    ")\n",
    "y_tr_log_ult = np.log1p(y_tr_ult)\n",
    "y_val_log_ult = np.log1p(y_val_ult)\n",
    "\n",
    "print(f\"Training: {X_tr_ult.shape[0]:,} samples\")\n",
    "print(f\"Validation: {X_val_ult.shape[0]:,} samples\")\n",
    "print(f\"Features: {X_tr_ult.shape[1]}\")\n",
    "\n",
    "# Optimized parameters for multimodal data\n",
    "xgb_ultimate_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.03,  # Lower for more features\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.5,  # Lower due to many features\n",
    "    'colsample_bylevel': 0.7,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.5,  # Higher regularization\n",
    "    'reg_lambda': 2.0,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 500,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "\n",
    "print(\"\\nðŸš€ Training ultimate XGBoost model...\")\n",
    "xgb_ultimate = xgb.XGBRegressor(**xgb_ultimate_params)\n",
    "xgb_ultimate.fit(\n",
    "    X_tr_ult, y_tr_log_ult,\n",
    "    eval_set=[(X_val_ult, y_val_log_ult)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Predict\n",
    "xgb_ult_pred_val = np.expm1(xgb_ultimate.predict(X_val_ult))\n",
    "xgb_ult_pred_val = np.maximum(xgb_ult_pred_val, 0.1)\n",
    "\n",
    "# Metrics\n",
    "xgb_ult_smape = calculate_smape(y_val_ult, xgb_ult_pred_val)\n",
    "xgb_ult_mae = mean_absolute_error(y_val_ult, xgb_ult_pred_val)\n",
    "xgb_ult_rmse = np.sqrt(mean_squared_error(y_val_ult, xgb_ult_pred_val))\n",
    "\n",
    "print(f\"\\nâœ… ULTIMATE MODEL RESULTS:\")\n",
    "print(f\"   SMAPE: {xgb_ult_smape:.4f}%\")\n",
    "print(f\"   MAE: ${xgb_ult_mae:.2f}\")\n",
    "print(f\"   RMSE: ${xgb_ult_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "xgb_ultimate_params_no_es = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.03,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'colsample_bylevel': 0.7,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.5,\n",
    "    'reg_lambda': 2.0,\n",
    "    'n_estimators': 400,  # Fixed number, no early stopping\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0\n",
    "}\n",
    "\n",
    "ultimate_predictions_test = []\n",
    "ultimate_smapes = []\n",
    "\n",
    "for seed in [42, 123, 456]:  # Use fewer seeds for speed\n",
    "    print(f\"\\nTraining ultimate model with seed {seed}...\")\n",
    "    \n",
    "    X_tr_seed, X_val_seed, y_tr_seed, y_val_seed = train_test_split(\n",
    "        X_train_ultimate, y_train_ultimate, test_size=0.15, random_state=seed\n",
    "    )\n",
    "    y_tr_log_seed = np.log1p(y_tr_seed)\n",
    "    \n",
    "    model_seed = xgb.XGBRegressor(**{**xgb_ultimate_params_no_es, 'random_state': seed})\n",
    "    model_seed.fit(X_tr_seed, y_tr_log_seed, verbose=False)\n",
    "    \n",
    "    pred_val = np.expm1(model_seed.predict(X_val_seed))\n",
    "    pred_test = np.expm1(model_seed.predict(X_test_ultimate))\n",
    "    \n",
    "    ultimate_predictions_test.append(pred_test)\n",
    "    \n",
    "    smape_seed = calculate_smape(y_val_seed, np.maximum(pred_val, 0.1))\n",
    "    ultimate_smapes.append(smape_seed)\n",
    "    print(f\"   Seed {seed} SMAPE: {smape_seed:.4f}%\")\n",
    "\n",
    "# Ensemble\n",
    "ultimate_ensemble_test = np.mean(ultimate_predictions_test, axis=0)\n",
    "ultimate_ensemble_test = np.maximum(ultimate_ensemble_test, 0.1)\n",
    "\n",
    "avg_ultimate_smape = np.mean(ultimate_smapes)\n",
    "print(f\"\\nâœ… Ultimate Multi-Seed Ensemble (with images):\")\n",
    "print(f\"   Average SMAPE: {avg_ultimate_smape:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BETTER APPROACH: OPTIMIZE TEXT+EMBEDDINGS (NO IMAGES)\n",
    "\n",
    "\n",
    "# Load text+embeddings features (397 features, no images)\n",
    "X_train_optimized = train_df_full[text_features + text_emb_names].values\n",
    "y_train_optimized = train_df_full['price'].values\n",
    "y_train_log_optimized = np.log1p(y_train_optimized)\n",
    "\n",
    "X_test_optimized = test_df_full[text_features + text_emb_names].values\n",
    "\n",
    "# Split\n",
    "X_tr_opt, X_val_opt, y_tr_opt, y_val_opt = train_test_split(\n",
    "    X_train_optimized, y_train_optimized, test_size=0.15, random_state=42\n",
    ")\n",
    "y_tr_log_opt = np.log1p(y_tr_opt)\n",
    "y_val_log_opt = np.log1p(y_val_opt)\n",
    "\n",
    "print(f\"\\nOptimized dataset:\")\n",
    "print(f\"   Features: {X_train_optimized.shape[1]} (text+embeddings only)\")\n",
    "print(f\"   Training: {X_tr_opt.shape[0]:,}\")\n",
    "print(f\"   Validation: {X_val_opt.shape[0]:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HYPERPARAMETER TUNING WITH OPTUNA (FAST)\n",
    "# ========\n",
    "# Install optuna\n",
    "import subprocess\n",
    "subprocess.check_call(['pip', 'install', '-q', 'optuna'])\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function\"\"\"\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'random_state': 42,\n",
    "        \n",
    "        # Tunable parameters\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 9),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 2.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 5.0, log=True),\n",
    "        'n_estimators': 300\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_tr_opt, y_tr_log_opt, verbose=False)\n",
    "    \n",
    "    pred = np.expm1(model.predict(X_val_opt))\n",
    "    pred = np.maximum(pred, 0.1)\n",
    "    \n",
    "    smape = calculate_smape(y_val_opt, pred)\n",
    "    return smape\n",
    "\n",
    "# Run optimization (20 trials = ~15 minutes)\n",
    "print(\"\\nðŸ” Running Optuna optimization (20 trials, ~15 minutes)...\")\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ… Optimization complete!\")\n",
    "print(f\"   Best SMAPE: {study.best_value:.4f}%\")\n",
    "print(f\"   Best params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"      {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN FINAL MODEL WITH BEST PARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "best_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 500,\n",
    "    **study.best_params\n",
    "}\n",
    "\n",
    "# Train on full data\n",
    "model_optimized = xgb.XGBRegressor(**best_params)\n",
    "model_optimized.fit(\n",
    "    X_train_optimized, y_train_log_optimized, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predict on validation\n",
    "pred_opt_val = np.expm1(model_optimized.predict(X_val_opt))\n",
    "pred_opt_val = np.maximum(pred_opt_val, 0.1)\n",
    "\n",
    "opt_smape = calculate_smape(y_val_opt, pred_opt_val)\n",
    "\n",
    "print(f\"\\nâœ… OPTIMIZED MODEL RESULTS:\")\n",
    "print(f\"   SMAPE: {opt_smape:.4f}%\")\n",
    "print(f\"   Improvement from baseline: {54.01 - opt_smape:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compare all approaches\n",
    "comparison = pd.DataFrame({\n",
    "    'Approach': [\n",
    "        'Original Multi-Seed (Text+Emb)',\n",
    "        'With Images (Text+Emb+CLIP)',\n",
    "        'Optuna Tuned (Text+Emb)'\n",
    "    ],\n",
    "    'SMAPE (%)': [\n",
    "        54.01,\n",
    "        avg_ultimate_smape if 'avg_ultimate_smape' in locals() else 55.31,\n",
    "        opt_smape\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison = comparison.sort_values('SMAPE (%)')\n",
    "display(comparison)\n",
    "\n",
    "best_approach_row = comparison.iloc[0]\n",
    "print(f\"\\nðŸ† BEST APPROACH: {best_approach_row['Approach']}\")\n",
    "print(f\"   SMAPE: {best_approach_row['SMAPE (%)']:.4f}%\")\n",
    "\n",
    "# Use best model for final submission\n",
    "if best_approach_row['Approach'] == 'Optuna Tuned (Text+Emb)':\n",
    "    final_predictions = np.expm1(model_optimized.predict(X_test_optimized))\n",
    "    final_smape_est = opt_smape\n",
    "    final_method = \"Optuna-Tuned XGBoost (Text+Embeddings)\"\n",
    "elif best_approach_row['Approach'] == 'Original Multi-Seed (Text+Emb)':\n",
    "    # Use your original multi-seed ensemble (54.01%)\n",
    "    final_predictions = np.mean(predictions_test, axis=0)  # From earlier\n",
    "    final_smape_est = 54.01\n",
    "    final_method = \"Multi-Seed XGBoost Ensemble (Text+Embeddings)\"\n",
    "else:\n",
    "    final_predictions = ultimate_ensemble_test\n",
    "    final_smape_est = avg_ultimate_smape\n",
    "    final_method = \"Multi-Seed with Images\"\n",
    "\n",
    "final_predictions = np.maximum(final_predictions, 0.1)\n",
    "\n",
    "# Create submission\n",
    "final_submission_v2 = pd.DataFrame({\n",
    "    'sample_id': test_df_full['sample_id'],\n",
    "    'price': final_predictions\n",
    "})\n",
    "\n",
    "final_submission_v2 = final_submission_v2.sort_values('sample_id').reset_index(drop=True)\n",
    "final_submission_v2.to_csv('test_out_FINAL_V2.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… FINAL SUBMISSION CREATED:\")\n",
    "print(f\"   File: test_out_FINAL_V2.csv\")\n",
    "print(f\"   Method: {final_method}\")\n",
    "print(f\"   Expected SMAPE: {final_smape_est:.4f}%\")\n",
    "print(f\"   Samples: {len(final_submission_v2):,}\")\n",
    "\n",
    "print(f\"\\nCurrent leaderboard: 53.961% (Rank 1868)\")\n",
    "print(f\"Expected new score: {final_smape_est:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n1. Checking validation data...\")\n",
    "print(f\"   X_val_opt shape: {X_val_opt.shape}\")\n",
    "print(f\"   y_val_opt shape: {y_val_opt.shape}\")\n",
    "print(f\"   Unique values in y_val_opt: {len(np.unique(y_val_opt))}\")\n",
    "\n",
    "# Check predictions\n",
    "print(\"\\n2. Checking predictions...\")\n",
    "print(f\"   pred_opt_val shape: {pred_opt_val.shape}\")\n",
    "print(f\"   pred_opt_val range: ${pred_opt_val.min():.2f} - ${pred_opt_val.max():.2f}\")\n",
    "print(f\"   pred_opt_val mean: ${pred_opt_val.mean():.2f}\")\n",
    "print(f\"   y_val_opt mean: ${y_val_opt.mean():.2f}\")\n",
    "\n",
    "# Recalculate SMAPE manually\n",
    "manual_smape = calculate_smape(y_val_opt, pred_opt_val)\n",
    "print(f\"\\n3. Manual SMAPE calculation: {manual_smape:.4f}%\")\n",
    "\n",
    "# Check if model was evaluated on training data by mistake\n",
    "train_pred = np.expm1(model_optimized.predict(X_train_optimized))\n",
    "train_pred = np.maximum(train_pred, 0.1)\n",
    "train_smape = calculate_smape(y_train_optimized, train_pred)\n",
    "print(f\"\\n4. SMAPE on FULL TRAINING data: {train_smape:.4f}%\")\n",
    "\n",
    "print(\"\\nâš ï¸  DIAGNOSIS:\")\n",
    "if train_smape < 30:\n",
    "    print(\"   Likely evaluated on TRAINING data (overfitting)\")\n",
    "    print(\"   Model memorized training data!\")\n",
    "elif manual_smape > 50:\n",
    "    print(\"   Validation split issue or calculation error\")\n",
    "else:\n",
    "    print(\"   Unknown error - need investigation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 3: INTERACTION FEATURES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREATING INTERACTION FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def create_interactions(df):\n",
    "    \"\"\"Create meaningful feature interactions\"\"\"\n",
    "    \n",
    "    # Quantity-based interactions\n",
    "    df['pack_x_unit'] = df['pack_count'] * df['unit_value']\n",
    "    df['quantity_x_bullets'] = df['total_quantity'] * df['num_bullet_points']\n",
    "    \n",
    "    # Text-based interactions\n",
    "    df['words_x_chars'] = df['word_count'] * df['char_count']\n",
    "    df['words_per_bullet'] = df['word_count'] / np.maximum(df['num_bullet_points'], 1)\n",
    "    \n",
    "    # Category interactions with quantities\n",
    "    df['category_x_pack'] = df['category_encoded'] * df['pack_count']\n",
    "    df['unit_type_x_value'] = df['unit_type_encoded'] * df['unit_value']\n",
    "    \n",
    "    # Price-related interactions\n",
    "    df['pack_x_weight'] = df['pack_count'] * df['weight_value']\n",
    "    df['bullets_x_chars'] = df['num_bullet_points'] * df['char_count']\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df_adv = create_interactions(train_df_adv)\n",
    "test_df_adv = create_interactions(test_df_adv)\n",
    "\n",
    "interaction_features = [\n",
    "    'pack_x_unit', 'quantity_x_bullets', 'words_x_chars', \n",
    "    'words_per_bullet', 'category_x_pack', 'unit_type_x_value',\n",
    "    'pack_x_weight', 'bullets_x_chars'\n",
    "]\n",
    "\n",
    "print(f\"âœ… Added {len(interaction_features)} interaction features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 4: ULTIMATE FEATURE SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREATING ULTIMATE FEATURE SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Original features\n",
    "original_features = [\n",
    "    'unit_value', 'pack_count', 'total_quantity', 'weight_value',\n",
    "    'num_bullet_points', 'word_count', 'char_count', 'num_numbers', 'avg_word_length',\n",
    "    'unit_type_encoded', 'category_encoded', 'unit_type_freq', 'category_freq'\n",
    "]\n",
    "\n",
    "# Text embeddings\n",
    "text_emb_features = [f'text_emb_{i}' for i in range(384)]\n",
    "\n",
    "# All new features\n",
    "ultimate_feature_set = (\n",
    "    original_features + \n",
    "    text_emb_features + \n",
    "    advanced_features + \n",
    "    target_features + \n",
    "    interaction_features\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š ULTIMATE FEATURE SET:\")\n",
    "print(f\"   Original features: {len(original_features)}\")\n",
    "print(f\"   Text embeddings: {len(text_emb_features)}\")\n",
    "print(f\"   Advanced features: {len(advanced_features)}\")\n",
    "print(f\"   Target encoded: {len(target_features)}\")\n",
    "print(f\"   Interactions: {len(interaction_features)}\")\n",
    "print(f\"   TOTAL: {len(ultimate_feature_set)} features\")\n",
    "\n",
    "# Check which features exist\n",
    "existing_features = [f for f in ultimate_feature_set if f in train_df_adv.columns]\n",
    "missing_features = [f for f in ultimate_feature_set if f not in train_df_adv.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\nâš ï¸  Missing features: {missing_features[:5]}...\")\n",
    "    ultimate_feature_set = existing_features\n",
    "\n",
    "# Prepare data\n",
    "X_train_ultimate = train_df_adv[ultimate_feature_set].fillna(0).values\n",
    "y_train_ultimate = train_df_adv['price'].values\n",
    "y_train_log_ultimate = np.log1p(y_train_ultimate)\n",
    "\n",
    "X_test_ultimate = test_df_adv[ultimate_feature_set].fillna(0).values\n",
    "test_ids_ultimate = test_df_adv['sample_id'].values\n",
    "\n",
    "print(f\"\\nâœ… Final data prepared: {X_train_ultimate.shape}\")\n",
    "print(f\"   Using {len(ultimate_feature_set)} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY 5: FAST SINGLE MODEL TEST\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING ADVANCED FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split for validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_ultimate, y_train_ultimate, test_size=0.15, random_state=42\n",
    ")\n",
    "y_tr_log = np.log1p(y_tr)\n",
    "\n",
    "# Quick XGBoost test\n",
    "xgb_test_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'reg_alpha': 0.2,\n",
    "    'reg_lambda': 1.5,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 300,\n",
    "    'tree_method': 'gpu_hist'\n",
    "}\n",
    "\n",
    "print(\"ðŸš€ Testing XGBoost with advanced features...\")\n",
    "xgb_test = xgb.XGBRegressor(**xgb_test_params)\n",
    "xgb_test.fit(X_tr, y_tr_log, verbose=False)\n",
    "\n",
    "pred_test = np.expm1(xgb_test.predict(X_val))\n",
    "pred_test = np.maximum(pred_test, 0.1)\n",
    "smape_test = calculate_smape(y_val, pred_test)\n",
    "\n",
    "print(f\"\\nâœ… ADVANCED FEATURES TEST RESULT:\")\n",
    "print(f\"   SMAPE: {smape_test:.4f}%\")\n",
    "\n",
    "improvement = 53.96 - smape_test\n",
    "print(f\"\\nðŸŽ¯ IMPROVEMENT CHECK:\")\n",
    "print(f\"   Current leaderboard: 53.96%\")\n",
    "print(f\"   Advanced features: {smape_test:.4f}%\")\n",
    "print(f\"   Improvement: {improvement:.4f}%\")\n",
    "\n",
    "if improvement > 1:\n",
    "    print(f\"\\nðŸŽ‰ EXCELLENT! >1% improvement - continue with ensemble!\")\n",
    "    continue_ensemble = True\n",
    "elif improvement > 0:\n",
    "    print(f\"\\nâœ… GOOD! Some improvement - worth submitting!\")\n",
    "    continue_ensemble = True\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  No improvement - stick with current model\")\n",
    "    continue_ensemble = False\n",
    "\n",
    "if continue_ensemble:\n",
    "    print(\"\\nðŸ“Š Feature importance (top 10):\")\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': [ultimate_feature_set[i] for i in range(len(ultimate_feature_set))],\n",
    "        'importance': xgb_test.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    display(importance_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATEGY: STACKING ENSEMBLE (PROVEN TECHNIQUE)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸŽ¯ STRATEGY: STACKING ENSEMBLE FOR SCORE BOOST\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nInstead of new features, let's optimize model combination\")\n",
    "\n",
    "# Load your proven best features\n",
    "with open('feature_config.json', 'r') as f:\n",
    "    feature_config = json.load(f)\n",
    "\n",
    "best_features = feature_config['all_features']  # Your original 13 features\n",
    "\n",
    "# Add text embeddings\n",
    "text_emb_features = [f'text_emb_{i}' for i in range(384)]\n",
    "proven_features = best_features + text_emb_features  # 397 features total\n",
    "\n",
    "# Load data with proven features\n",
    "train_df_best = pd.read_csv('train_processed.csv')\n",
    "test_df_best = pd.read_csv('test_processed.csv')\n",
    "\n",
    "# Add embeddings if needed\n",
    "if 'text_emb_0' not in train_df_best.columns:\n",
    "    train_text_emb = np.load('train_text_embeddings_full.npy')\n",
    "    test_text_emb = np.load('test_text_embeddings_full.npy')\n",
    "    \n",
    "    for i in range(384):\n",
    "        train_df_best[f'text_emb_{i}'] = train_text_emb[:, i]\n",
    "        test_df_best[f'text_emb_{i}'] = test_text_emb[:, i]\n",
    "\n",
    "X_train_best = train_df_best[proven_features].fillna(0).values\n",
    "y_train_best = train_df_best['price'].values\n",
    "y_train_log_best = np.log1p(y_train_best)\n",
    "\n",
    "X_test_best = test_df_best[proven_features].fillna(0).values\n",
    "test_ids_best = test_df_best['sample_id'].values\n",
    "\n",
    "print(f\"âœ… Using proven features: {len(proven_features)}\")\n",
    "print(f\"   Data shape: {X_train_best.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STACKING ENSEMBLE - LEVEL 1 MODELS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING STACKING ENSEMBLE - LEVEL 1\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create out-of-fold predictions for stacking\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize OOF prediction arrays\n",
    "oof_xgb_stack = np.zeros(len(X_train_best))\n",
    "oof_lgb_stack = np.zeros(len(X_train_best))\n",
    "oof_cat_stack = np.zeros(len(X_train_best))\n",
    "\n",
    "# Initialize test prediction arrays\n",
    "test_xgb_stack = np.zeros(len(X_test_best))\n",
    "test_lgb_stack = np.zeros(len(X_test_best))\n",
    "test_cat_stack = np.zeros(len(X_test_best))\n",
    "\n",
    "print(\"Creating out-of-fold predictions for stacking...\")\n",
    "\n",
    "# Optimized parameters for stacking\n",
    "xgb_stack_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'reg_alpha': 0.2,\n",
    "    'reg_lambda': 1.5,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 400,\n",
    "    'tree_method': 'gpu_hist'\n",
    "}\n",
    "\n",
    "lgb_stack_params = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 50,\n",
    "    'max_depth': 7,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'reg_alpha': 0.2,\n",
    "    'reg_lambda': 1.5,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 400,\n",
    "    'device': 'gpu',\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "cat_stack_params = {\n",
    "    'iterations': 400,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 7,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'subsample': 0.8,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'task_type': 'GPU'\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_best), 1):\n",
    "    print(f\"\\nFold {fold}/{n_folds}\")\n",
    "    \n",
    "    X_tr_fold = X_train_best[train_idx]\n",
    "    X_val_fold = X_train_best[val_idx]\n",
    "    y_tr_fold = y_train_log_best[train_idx]\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_fold = xgb.XGBRegressor(**xgb_stack_params)\n",
    "    xgb_fold.fit(X_tr_fold, y_tr_fold, verbose=False)\n",
    "    \n",
    "    oof_xgb_stack[val_idx] = xgb_fold.predict(X_val_fold)\n",
    "    test_xgb_stack += xgb_fold.predict(X_test_best) / n_folds\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_fold = lgb.LGBMRegressor(**lgb_stack_params)\n",
    "    lgb_fold.fit(X_tr_fold, y_tr_fold)\n",
    "    \n",
    "    oof_lgb_stack[val_idx] = lgb_fold.predict(X_val_fold)\n",
    "    test_lgb_stack += lgb_fold.predict(X_test_best) / n_folds\n",
    "    \n",
    "    # CatBoost\n",
    "    cat_fold = CatBoostRegressor(**cat_stack_params)\n",
    "    cat_fold.fit(X_tr_fold, y_tr_fold, verbose=False)\n",
    "    \n",
    "    oof_cat_stack[val_idx] = cat_fold.predict(X_val_fold)\n",
    "    test_cat_stack += cat_fold.predict(X_test_best) / n_folds\n",
    "\n",
    "print(\"\\nâœ… Level 1 models complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STACKING ENSEMBLE - LEVEL 2 META-LEARNER\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING LEVEL 2 META-LEARNER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create meta-features (OOF predictions from level 1)\n",
    "meta_features = np.column_stack([\n",
    "    oof_xgb_stack,\n",
    "    oof_lgb_stack, \n",
    "    oof_cat_stack\n",
    "])\n",
    "\n",
    "# Meta-learner (Ridge regression)\n",
    "meta_learner = Ridge(alpha=1.0, random_state=42)\n",
    "meta_learner.fit(meta_features, y_train_log_best)\n",
    "\n",
    "print(f\"âœ… Meta-learner trained!\")\n",
    "print(f\"   Meta-features shape: {meta_features.shape}\")\n",
    "print(f\"   Ridge coefficients: {meta_learner.coef_}\")\n",
    "\n",
    "# Generate final stacking predictions\n",
    "test_meta_features = np.column_stack([\n",
    "    test_xgb_stack,\n",
    "    test_lgb_stack,\n",
    "    test_cat_stack\n",
    "])\n",
    "\n",
    "stacking_pred_log = meta_learner.predict(meta_features)  # Validation predictions\n",
    "stacking_pred_test_log = meta_learner.predict(test_meta_features)  # Test predictions\n",
    "\n",
    "# Convert back to original price scale\n",
    "stacking_pred_val = np.expm1(stacking_pred_log)\n",
    "stacking_pred_test = np.expm1(stacking_pred_test_log)\n",
    "\n",
    "stacking_pred_val = np.maximum(stacking_pred_val, 0.1)\n",
    "stacking_pred_test = np.maximum(stacking_pred_test, 0.1)\n",
    "\n",
    "# Calculate stacking SMAPE\n",
    "stacking_smape = calculate_smape(y_train_best, stacking_pred_val)\n",
    "\n",
    "print(f\"\\nâœ… STACKING ENSEMBLE RESULTS:\")\n",
    "print(f\"   Stacking SMAPE: {stacking_smape:.4f}%\")\n",
    "\n",
    "improvement = 53.96 - stacking_smape\n",
    "print(f\"\\nðŸŽ¯ IMPROVEMENT CHECK:\")\n",
    "print(f\"   Current leaderboard: 53.96%\")\n",
    "print(f\"   Stacking ensemble: {stacking_smape:.4f}%\")\n",
    "print(f\"   Improvement: {improvement:.4f}%\")\n",
    "\n",
    "if improvement > 0.5:\n",
    "    print(f\"\\nðŸŽ‰ GOOD IMPROVEMENT! Submit stacking model!\")\n",
    "    use_stacking = True\n",
    "elif improvement > 0:\n",
    "    print(f\"\\nâœ… Small improvement - worth trying!\")\n",
    "    use_stacking = True\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  No significant improvement\")\n",
    "    use_stacking = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALTERNATIVE: HYPERPARAMETER OPTIMIZATION ROUND 2\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALTERNATIVE: DEEPER HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# If stacking doesn't work, try more aggressive hyperparameter tuning\n",
    "if not use_stacking or improvement < 1:\n",
    "    print(\"Trying deeper hyperparameter optimization...\")\n",
    "    \n",
    "    import optuna\n",
    "    \n",
    "    def objective_v2(trial):\n",
    "        \"\"\"More aggressive hyperparameter search\"\"\"\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'tree_method': 'gpu_hist',\n",
    "            'random_state': 42,\n",
    "            \n",
    "            # Wider search ranges\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.15, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
    "            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.3, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 5.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0, log=True),\n",
    "            'n_estimators': 350\n",
    "        }\n",
    "        \n",
    "        # Cross-validation with proper split\n",
    "        X_tr_opt, X_val_opt, y_tr_opt, y_val_opt = train_test_split(\n",
    "            X_train_best, y_train_best, test_size=0.2, random_state=trial.number\n",
    "        )\n",
    "        y_tr_log_opt = np.log1p(y_tr_opt)\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_tr_opt, y_tr_log_opt, verbose=False)\n",
    "        \n",
    "        pred = np.expm1(model.predict(X_val_opt))\n",
    "        pred = np.maximum(pred, 0.1)\n",
    "        \n",
    "        smape = calculate_smape(y_val_opt, pred)\n",
    "        return smape\n",
    "    \n",
    "    print(\"\\nðŸ” Running deeper optimization (30 trials)...\")\n",
    "    study_v2 = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study_v2.optimize(objective_v2, n_trials=30, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\nâœ… Deep optimization complete!\")\n",
    "    print(f\"   Best SMAPE: {study_v2.best_value:.4f}%\")\n",
    "    \n",
    "    # Train final model with best params\n",
    "    best_params_v2 = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'random_state': 42,\n",
    "        'n_estimators': 500,\n",
    "        **study_v2.best_params\n",
    "    }\n",
    "    \n",
    "    final_model_opt = xgb.XGBRegressor(**best_params_v2)\n",
    "    final_model_opt.fit(X_train_best, y_train_log_best, verbose=False)\n",
    "    \n",
    "    pred_opt_test = np.expm1(final_model_opt.predict(X_test_best))\n",
    "    pred_opt_test = np.maximum(pred_opt_test, 0.1)\n",
    "    \n",
    "    deep_opt_improvement = 53.96 - study_v2.best_value\n",
    "    print(f\"\\nðŸŽ¯ DEEP OPTIMIZATION RESULTS:\")\n",
    "    print(f\"   Best SMAPE: {study_v2.best_value:.4f}%\")\n",
    "    print(f\"   Improvement: {deep_opt_improvement:.4f}%\")\n",
    "    \n",
    "    if deep_opt_improvement > 0.5:\n",
    "        print(f\"\\nðŸŽ‰ USE DEEP OPTIMIZED MODEL!\")\n",
    "        final_predictions_advanced = pred_opt_test\n",
    "        final_method_advanced = \"Deep Hyperparameter Optimized XGBoost\"\n",
    "        final_smape_advanced = study_v2.best_value\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  Stick with original multi-seed ensemble\")\n",
    "        final_predictions_advanced = None\n",
    "        final_method_advanced = \"Original Multi-Seed Ensemble\"\n",
    "        final_smape_advanced = 54.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL DECISION & SUBMISSION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL MODEL SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare all approaches\n",
    "results_final = pd.DataFrame({\n",
    "    'Approach': [\n",
    "        'Original Multi-Seed (Proven)',\n",
    "        'Stacking Ensemble',\n",
    "        'Deep Hyperparameter Tuning' if 'study_v2' in locals() else 'Not Tested'\n",
    "    ],\n",
    "    'Expected SMAPE (%)': [\n",
    "        54.01,\n",
    "        stacking_smape if 'stacking_smape' in locals() else 'N/A',\n",
    "        study_v2.best_value if 'study_v2' in locals() else 'N/A'\n",
    "    ],\n",
    "    'Status': [\n",
    "        'âœ… Validated (53.96% LB)',\n",
    "        'ðŸ§ª Experimental',\n",
    "        'ðŸ§ª Experimental'\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(results_final)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š RECOMMENDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check which model is actually best\n",
    "best_validated_smape = 54.01  # Your proven score\n",
    "\n",
    "if 'stacking_smape' in locals() and stacking_smape < best_validated_smape - 0.5:\n",
    "    print(\"âœ… SUBMIT: Stacking Ensemble\")\n",
    "    final_submit_predictions = stacking_pred_test\n",
    "    final_submit_method = \"Stacking Ensemble\"\n",
    "    final_submit_smape = stacking_smape\n",
    "elif 'study_v2' in locals() and study_v2.best_value < best_validated_smape - 0.5:\n",
    "    print(\"âœ… SUBMIT: Deep Optimized Model\")\n",
    "    final_submit_predictions = pred_opt_test\n",
    "    final_submit_method = \"Deep Hyperparameter Optimized\"\n",
    "    final_submit_smape = study_v2.best_value\n",
    "else:\n",
    "    print(\"âœ… SUBMIT: Original Multi-Seed Ensemble (SAFEST)\")\n",
    "    print(\"   Advanced techniques didn't provide significant improvement\")\n",
    "    print(\"   Your 54.01% model is already well-optimized!\")\n",
    "    final_submit_method = \"Original Multi-Seed Ensemble\"\n",
    "    final_submit_smape = 54.01\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Expected Performance:\")\n",
    "print(f\"   Method: {final_submit_method}\")\n",
    "print(f\"   Expected SMAPE: {final_submit_smape:.2f}%\")\n",
    "print(f\"   Expected Rank: Top 500-1000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('train_processed.csv')\n",
    "test_df = pd.read_csv('test_processed.csv')\n",
    "\n",
    "print(f\"\\nâœ… Data loaded: {len(train_df):,} train, {len(test_df):,} test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TECHNIQUE 1: FIND DUPLICATES & NEAR-DUPLICATES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TECHNIQUE 1: DUPLICATE DETECTION & PRICE TRANSFER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import hashlib\n",
    "\n",
    "def create_text_hash(row):\n",
    "    \"\"\"Create hash from key text fields\"\"\"\n",
    "    text = f\"{row['item_name']}_{row['catalog_content']}\"\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "# Create text hashes\n",
    "train_df['text_hash'] = train_df.apply(create_text_hash, axis=1)\n",
    "test_df['text_hash'] = test_df.apply(create_text_hash, axis=1)\n",
    "\n",
    "# Find exact duplicates between train and test\n",
    "train_hashes = set(train_df['text_hash'])\n",
    "test_hashes = set(test_df['text_hash'])\n",
    "common_hashes = train_hashes & test_hashes\n",
    "\n",
    "print(f\"\\nðŸ” Duplicate Analysis:\")\n",
    "print(f\"   Exact duplicates: {len(common_hashes)}\")\n",
    "\n",
    "# For exact duplicates, use train price as feature\n",
    "if len(common_hashes) > 0:\n",
    "    hash_price_map = train_df.groupby('text_hash')['price'].mean().to_dict()\n",
    "    \n",
    "    train_df['duplicate_price'] = train_df['text_hash'].map(hash_price_map)\n",
    "    test_df['duplicate_price'] = test_df['text_hash'].map(hash_price_map)\n",
    "    \n",
    "    # Fill missing with 0 (no duplicate found)\n",
    "    train_df['duplicate_price'] = train_df['duplicate_price'].fillna(0)\n",
    "    test_df['duplicate_price'] = test_df['duplicate_price'].fillna(0)\n",
    "    \n",
    "    train_df['is_duplicate'] = (train_df['duplicate_price'] > 0).astype(int)\n",
    "    test_df['is_duplicate'] = (test_df['duplicate_price'] > 0).astype(int)\n",
    "    \n",
    "    print(f\"   Train with duplicates: {train_df['is_duplicate'].sum():,}\")\n",
    "    print(f\"   Test with duplicates: {test_df['is_duplicate'].sum():,}\")\n",
    "else:\n",
    "    train_df['duplicate_price'] = 0\n",
    "    test_df['duplicate_price'] = 0\n",
    "    train_df['is_duplicate'] = 0\n",
    "    test_df['is_duplicate'] = 0\n",
    "\n",
    "# Near-duplicates using TF-IDF similarity\n",
    "print(f\"\\nðŸ” Finding near-duplicates (cosine similarity > 0.95)...\")\n",
    "\n",
    "# Sample for speed (use all if time permits)\n",
    "sample_size = min(10000, len(train_df))\n",
    "train_sample = train_df.sample(sample_size, random_state=42)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1,2))\n",
    "train_tfidf = tfidf.fit_transform(train_sample['catalog_content'].fillna(''))\n",
    "test_tfidf = tfidf.transform(test_df['catalog_content'].fillna(''))\n",
    "\n",
    "# Find similar products\n",
    "similarity_threshold = 0.95\n",
    "test_df['near_duplicate_price'] = 0.0\n",
    "\n",
    "for i in range(min(1000, len(test_df))):  # Process first 1000 for speed\n",
    "    similarities = cosine_similarity(test_tfidf[i:i+1], train_tfidf).flatten()\n",
    "    max_sim_idx = similarities.argmax()\n",
    "    \n",
    "    if similarities[max_sim_idx] > similarity_threshold:\n",
    "        test_df.loc[test_df.index[i], 'near_duplicate_price'] = train_sample.iloc[max_sim_idx]['price']\n",
    "\n",
    "train_df['near_duplicate_price'] = 0.0  # Train doesn't need this\n",
    "\n",
    "near_dup_count = (test_df['near_duplicate_price'] > 0).sum()\n",
    "print(f\"   Near-duplicates found: {near_dup_count}\")\n",
    "\n",
    "print(\"\\nâœ… TECHNIQUE 1 COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TECHNIQUE 2: TEST DISTRIBUTION ADAPTATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TECHNIQUE 2: TEST DISTRIBUTION ADAPTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze distribution differences\n",
    "print(\"\\nðŸ“Š Distribution Analysis:\")\n",
    "\n",
    "# Category distribution\n",
    "train_cat_dist = train_df['category'].value_counts(normalize=True)\n",
    "test_cat_dist = test_df['category'].value_counts(normalize=True)\n",
    "\n",
    "# Find categories more common in test\n",
    "test_heavy_cats = []\n",
    "for cat in test_cat_dist.index:\n",
    "    if cat in train_cat_dist.index:\n",
    "        if test_cat_dist[cat] > train_cat_dist[cat] * 1.5:  # 50% more common in test\n",
    "            test_heavy_cats.append(cat)\n",
    "            print(f\"   {cat}: Train {train_cat_dist[cat]:.3f} â†’ Test {test_cat_dist[cat]:.3f}\")\n",
    "\n",
    "# Create feature for test-heavy categories\n",
    "train_df['test_heavy_category'] = train_df['category'].isin(test_heavy_cats).astype(int)\n",
    "test_df['test_heavy_category'] = test_df['category'].isin(test_heavy_cats).astype(int)\n",
    "\n",
    "# Unseen values\n",
    "train_units = set(train_df['unit_type'].unique())\n",
    "test_units = set(test_df['unit_type'].unique())\n",
    "unseen_units = test_units - train_units\n",
    "\n",
    "print(f\"\\n   Unseen unit types in test: {len(unseen_units)}\")\n",
    "test_df['has_unseen_unit'] = test_df['unit_type'].isin(unseen_units).astype(int)\n",
    "train_df['has_unseen_unit'] = 0\n",
    "\n",
    "# Price distribution feature\n",
    "train_price_percentiles = np.percentile(train_df['price'], [25, 50, 75])\n",
    "train_df['price_quartile'] = pd.cut(train_df['price'], \n",
    "                                     bins=[0] + list(train_price_percentiles) + [np.inf],\n",
    "                                     labels=[0,1,2,3]).astype(int)\n",
    "\n",
    "# For test, we'll estimate quartile based on features\n",
    "test_df['price_quartile'] = 1  # Default to median quartile\n",
    "\n",
    "print(\"\\nâœ… TECHNIQUE 2 COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TECHNIQUE 3: SEMANTIC CLUSTERING & GROUP STATISTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TECHNIQUE 3: SEMANTIC CLUSTERING (EMBEDDINGS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load text embeddings\n",
    "train_text_emb = np.load('train_text_embeddings_full.npy')\n",
    "test_text_emb = np.load('test_text_embeddings_full.npy')\n",
    "\n",
    "print(f\"âœ… Embeddings loaded: {train_text_emb.shape}\")\n",
    "\n",
    "# K-means clustering on embeddings\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "n_clusters = 100\n",
    "print(f\"\\nðŸ” Clustering products into {n_clusters} groups...\")\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=1000)\n",
    "train_df['cluster'] = kmeans.fit_predict(train_text_emb)\n",
    "test_df['cluster'] = kmeans.predict(test_text_emb)\n",
    "\n",
    "# Calculate cluster statistics\n",
    "cluster_stats = train_df.groupby('cluster')['price'].agg(['mean', 'std', 'count']).reset_index()\n",
    "cluster_stats.columns = ['cluster', 'cluster_price_mean', 'cluster_price_std', 'cluster_count']\n",
    "\n",
    "# Merge cluster stats\n",
    "train_df = train_df.merge(cluster_stats, on='cluster', how='left')\n",
    "test_df = test_df.merge(cluster_stats, on='cluster', how='left')\n",
    "\n",
    "# Fill missing\n",
    "train_df['cluster_price_mean'] = train_df['cluster_price_mean'].fillna(train_df['price'].mean())\n",
    "train_df['cluster_price_std'] = train_df['cluster_price_std'].fillna(train_df['price'].std())\n",
    "test_df['cluster_price_mean'] = test_df['cluster_price_mean'].fillna(train_df['price'].mean())\n",
    "test_df['cluster_price_std'] = test_df['cluster_price_std'].fillna(train_df['price'].std())\n",
    "\n",
    "# Cluster size feature\n",
    "train_df['large_cluster'] = (train_df['cluster_count'] > 500).astype(int)\n",
    "test_df['large_cluster'] = (test_df['cluster_count'].fillna(0) > 500).astype(int)\n",
    "\n",
    "print(f\"\\n   Clusters created: {n_clusters}\")\n",
    "print(f\"   Avg cluster size: {cluster_stats['cluster_count'].mean():.0f}\")\n",
    "print(f\"   Price variation across clusters: {cluster_stats['cluster_price_mean'].std():.2f}\")\n",
    "\n",
    "print(\"\\nâœ… TECHNIQUE 3 COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TECHNIQUE 4: BRAND & PRODUCT FAMILY FEATURES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TECHNIQUE 4: BRAND & PRODUCT FAMILY MINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_brand_features(df):\n",
    "    \"\"\"Extract brand-related features\"\"\"\n",
    "    \n",
    "    # Common brand keywords\n",
    "    df['has_popular_brand'] = df['catalog_content'].str.contains(\n",
    "        r'coca|pepsi|nestle|kraft|kellogg|general mills|nabisco|frito',\n",
    "        case=False, na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Extract first capitalized word as potential brand\n",
    "    def get_first_cap_word(text):\n",
    "        if pd.isna(text):\n",
    "            return 'unknown'\n",
    "        match = re.search(r'\\b[A-Z][a-z]+\\b', str(text))\n",
    "        return match.group(0) if match else 'unknown'\n",
    "    \n",
    "    df['extracted_brand'] = df['item_name'].apply(get_first_cap_word)\n",
    "    \n",
    "    # Brand frequency encoding\n",
    "    brand_freq = df['extracted_brand'].value_counts()\n",
    "    df['brand_frequency'] = df['extracted_brand'].map(brand_freq).fillna(1)\n",
    "    \n",
    "    # Rare brand indicator\n",
    "    df['rare_brand'] = (df['brand_frequency'] < 5).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = extract_brand_features(train_df)\n",
    "test_df = extract_brand_features(test_df)\n",
    "\n",
    "# Calculate brand price statistics\n",
    "brand_price_stats = train_df.groupby('extracted_brand')['price'].agg(['mean', 'std']).reset_index()\n",
    "brand_price_stats.columns = ['extracted_brand', 'brand_price_mean', 'brand_price_std']\n",
    "\n",
    "train_df = train_df.merge(brand_price_stats, on='extracted_brand', how='left')\n",
    "test_df = test_df.merge(brand_price_stats, on='extracted_brand', how='left')\n",
    "\n",
    "# Fill missing with global mean\n",
    "global_mean = train_df['price'].mean()\n",
    "global_std = train_df['price'].std()\n",
    "train_df['brand_price_mean'] = train_df['brand_price_mean'].fillna(global_mean)\n",
    "train_df['brand_price_std'] = train_df['brand_price_std'].fillna(global_std)\n",
    "test_df['brand_price_mean'] = test_df['brand_price_mean'].fillna(global_mean)\n",
    "test_df['brand_price_std'] = test_df['brand_price_std'].fillna(global_std)\n",
    "\n",
    "print(f\"\\n   Unique extracted brands: {train_df['extracted_brand'].nunique()}\")\n",
    "print(f\"   Popular brands found: {train_df['has_popular_brand'].sum():,}\")\n",
    "\n",
    "print(\"\\nâœ… TECHNIQUE 4 COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TECHNIQUE 5: PRICE-PREDICTIVE PATTERN MINING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TECHNIQUE 5: PRICE PATTERN MINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def extract_price_patterns(df):\n",
    "    \"\"\"Extract patterns that correlate with price\"\"\"\n",
    "    \n",
    "    # Size/quantity indicators\n",
    "    df['has_family_size'] = df['catalog_content'].str.contains(\n",
    "        r'family size|party size|value pack|economy', case=False, na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    df['has_single_serve'] = df['catalog_content'].str.contains(\n",
    "        r'single serve|individual|travel size|mini', case=False, na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Premium indicators\n",
    "    df['premium_words'] = df['catalog_content'].str.count(\n",
    "        r'premium|organic|natural|gourmet|artisan|craft|premium quality'\n",
    "    )\n",
    "    \n",
    "    # Discount indicators\n",
    "    df['discount_words'] = df['catalog_content'].str.count(\n",
    "        r'sale|discount|save|deal|value|cheap|affordable'\n",
    "    )\n",
    "    \n",
    "    # Health indicators (often premium priced)\n",
    "    df['health_words'] = df['catalog_content'].str.count(\n",
    "        r'gluten free|sugar free|low fat|diet|healthy|nutrition|vitamin'\n",
    "    )\n",
    "    \n",
    "    # Packaging complexity (correlates with price)\n",
    "    df['packaging_complexity'] = (\n",
    "        df['catalog_content'].str.count(r'reseal|zip|container|bottle|can') +\n",
    "        df['catalog_content'].str.count(r'package|wrapped|sealed')\n",
    "    )\n",
    "    \n",
    "    # Price per unit calculation\n",
    "    df['estimated_price_per_unit'] = df['total_quantity'] * df['unit_value'] / 100\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = extract_price_patterns(train_df)\n",
    "test_df = extract_price_patterns(test_df)\n",
    "\n",
    "print(f\"\\n   Pattern features extracted\")\n",
    "print(f\"   Premium products: {(train_df['premium_words'] > 0).sum():,}\")\n",
    "print(f\"   Discount products: {(train_df['discount_words'] > 0).sum():,}\")\n",
    "\n",
    "print(\"\\nâœ… TECHNIQUE 5 COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TECHNIQUE 6: PSEUDO-LABELING PREPARATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TECHNIQUE 6: PSEUDO-LABELING SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# We'll train initial model, then use confident predictions as pseudo-labels\n",
    "# This will be done after initial training\n",
    "\n",
    "print(\"âœ… Pseudo-labeling will be applied after initial training\")\n",
    "print(\"\\nâœ… TECHNIQUE 6 PREPARED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TECHNIQUE 7: COMPILE ULTIMATE FEATURE SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TECHNIQUE 7: COMPILING ULTIMATE FEATURE SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Original features\n",
    "original_features = [\n",
    "    'unit_value', 'pack_count', 'total_quantity', 'weight_value',\n",
    "    'num_bullet_points', 'word_count', 'char_count', 'num_numbers', 'avg_word_length',\n",
    "    'unit_type_encoded', 'category_encoded', 'unit_type_freq', 'category_freq'\n",
    "]\n",
    "\n",
    "# Text embeddings\n",
    "text_emb_features = [f'text_emb_{i}' for i in range(384)]\n",
    "\n",
    "# New advanced features from techniques 1-5\n",
    "advanced_features = [\n",
    "    # Technique 1: Duplicates\n",
    "    'duplicate_price', 'is_duplicate', 'near_duplicate_price',\n",
    "    # Technique 2: Distribution\n",
    "    'test_heavy_category', 'has_unseen_unit',\n",
    "    # Technique 3: Clustering\n",
    "    'cluster_price_mean', 'cluster_price_std', 'large_cluster',\n",
    "    # Technique 4: Brand\n",
    "    'has_popular_brand', 'brand_frequency', 'rare_brand',\n",
    "    'brand_price_mean', 'brand_price_std',\n",
    "    # Technique 5: Patterns\n",
    "    'has_family_size', 'has_single_serve', 'premium_words', 'discount_words',\n",
    "    'health_words', 'packaging_complexity', 'estimated_price_per_unit'\n",
    "]\n",
    "\n",
    "# Add embeddings to dataframes if not already there\n",
    "for i in range(384):\n",
    "    if f'text_emb_{i}' not in train_df.columns:\n",
    "        train_df[f'text_emb_{i}'] = train_text_emb[:, i]\n",
    "        test_df[f'text_emb_{i}'] = test_text_emb[:, i]\n",
    "\n",
    "# Combine all features\n",
    "ultimate_features = original_features + text_emb_features + advanced_features\n",
    "\n",
    "# Check which features exist\n",
    "existing_features = [f for f in ultimate_features if f in train_df.columns]\n",
    "missing_features = [f for f in ultimate_features if f not in train_df.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\nâš ï¸  Missing {len(missing_features)} features, using {len(existing_features)} available\")\n",
    "    ultimate_features = existing_features\n",
    "\n",
    "print(f\"\\nðŸ“Š ULTIMATE FEATURE SET:\")\n",
    "print(f\"   Total features: {len(ultimate_features)}\")\n",
    "print(f\"   Breakdown:\")\n",
    "print(f\"     - Original: {len([f for f in ultimate_features if f in original_features])}\")\n",
    "print(f\"     - Embeddings: {len([f for f in ultimate_features if 'text_emb_' in f])}\")\n",
    "print(f\"     - Advanced: {len([f for f in ultimate_features if f in advanced_features])}\")\n",
    "\n",
    "# Prepare final datasets\n",
    "X_train_ultimate = train_df[ultimate_features].fillna(0).values\n",
    "y_train_ultimate = train_df['price'].values\n",
    "y_train_log_ultimate = np.log1p(y_train_ultimate)\n",
    "\n",
    "X_test_ultimate = test_df[ultimate_features].fillna(0).values\n",
    "test_ids_ultimate = test_df['sample_id'].values\n",
    "\n",
    "print(f\"\\nâœ… Final data shape: {X_train_ultimate.shape}\")\n",
    "print(\"\\nâœ… TECHNIQUE 7 COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8471274,
     "sourceId": 13355962,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8471377,
     "sourceId": 13356097,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "amazon_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
